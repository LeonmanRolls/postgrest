{
    "docs": [
        {
            "location": "/", 
            "text": ".videoWrapper {\n  position: relative;\n  padding-bottom: 56.25%; /* 16:9 */\n  padding-top: 25px;\n  height: 0;\n}\n.videoWrapper iframe {\n  position: absolute;\n  top: 0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\n\n\n\n\n\n\nIntroduction\n\n\nPostgREST is a standalone web server that turns your database directly into a RESTful API. The structural constraints and permissions in the database determine the API endpoints and operations.\n\n\nThis guide explains how to install the software and provides practical examples of its use. You'll learn how to build a fast, versioned, secure API and how to deploy it to production.\n\n\nThe project has a friendly and growing community. Here are some ways to get help or get involved:\n\n\n\n\nThe project \nchat room\n\n\nReport or search \nissues\n\n\n\n\nMotivation\n\n\nUsing PostgREST is an alternative to manual CRUD programming. Custom API servers suffer problems. Writing business logic often duplicates, ignores or hobbles database structure. Object-relational mapping is a leaky abstraction leading to slow imperative code. The PostgREST philosophy establishes a single declarative source of truth: the data itself.\n\n\nDeclarative Programming\n\n\nIt's easier to ask Postgres to join data for you and let its query planner figure out the details than to loop through rows yourself. It's easier to assign permissions to db objects than to add guards in controllers. (This is especially true for cascading permissions in data dependencies.) It's easier set constraints than to litter code with sanity checks.\n\n\nLeakproof Abstraction\n\n\nThere is no ORM involved. Creating new views happens in SQL with known performance implications. A database administrator can now create an API from scratch with no custom programming. \n\n\nEmbracing the Relational Model\n\n\nIn 1970 E. F. Codd criticized the then-dominant hierarchical model of databases in his article \nA Relational Model of Data for Large Shared Data Banks\n. Reading the article reveals a striking similarity between hierarchical databases and nested http routes. With PostgREST we attempt to use flexible filtering and embedding rather than nested routes.\n\n\nOne Thing Well\n\n\nPostgREST has a focused scope. It works well with other tools like Nginx. This forces you to cleanly separate the data-centric CRUD operations from other concerns. Use a collection of sharp tools rather than building a big ball of mud.\n\n\nShared Improvements\n\n\nAs with any open source project, we all gain from features and fixes in the tool. It's more beneficial than improvements locked inextricably within custom codebases.\n\n\nIntro Video\n\n\nSome things have changed since this video was created but the basics are the same. Learn the big vision behind automating APIs.\n\n\n\n\n\n\n\n\n\nMyths\n\n\nYou have to make tons of stored procs and triggers\n\n\nModern PostgreSQL features like auto-updatable views and computed columns make this mostly unnecessary. Triggers do play a part, but generally not for irksome boilerplate. When they are required triggers are preferable to ad-hoc app code anyway, since the former work reliably for any codepath.\n\n\nExposing the database destroys encapsulation\n\n\nPostgREST does versioning through database schemas. This allows you to expose tables and views without making the app brittle. Underlying tables can be superseded and hidden behind public facing views. The chapter about versioning shows how to do this.\n\n\nConventions\n\n\nThis guide contains highlighted notes and tangential information interspersed with the text.\n\n\n\n    \nDesign Consideration\n\n\n    \nContains history which informed the current design. Sometimes it discusses unavoidable tradeoffs or a point of theory.\n\n\n\n\n\n\n    \nInvitation to Contribute\n\n\n    \nPoints out things we know we want to add or improve. They might give you ideas for ways to contribute to the project.\n\n\n\n\n\n\n    \nDeprecation Warning\n\n\n    \nAlerts you to features which will be removed in the next major (breaking) release.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "PostgREST is a standalone web server that turns your database directly into a RESTful API. The structural constraints and permissions in the database determine the API endpoints and operations.  This guide explains how to install the software and provides practical examples of its use. You'll learn how to build a fast, versioned, secure API and how to deploy it to production.  The project has a friendly and growing community. Here are some ways to get help or get involved:   The project  chat room  Report or search  issues", 
            "title": "Introduction"
        }, 
        {
            "location": "/#motivation", 
            "text": "Using PostgREST is an alternative to manual CRUD programming. Custom API servers suffer problems. Writing business logic often duplicates, ignores or hobbles database structure. Object-relational mapping is a leaky abstraction leading to slow imperative code. The PostgREST philosophy establishes a single declarative source of truth: the data itself.", 
            "title": "Motivation"
        }, 
        {
            "location": "/#declarative-programming", 
            "text": "It's easier to ask Postgres to join data for you and let its query planner figure out the details than to loop through rows yourself. It's easier to assign permissions to db objects than to add guards in controllers. (This is especially true for cascading permissions in data dependencies.) It's easier set constraints than to litter code with sanity checks.", 
            "title": "Declarative Programming"
        }, 
        {
            "location": "/#leakproof-abstraction", 
            "text": "There is no ORM involved. Creating new views happens in SQL with known performance implications. A database administrator can now create an API from scratch with no custom programming.", 
            "title": "Leakproof Abstraction"
        }, 
        {
            "location": "/#embracing-the-relational-model", 
            "text": "In 1970 E. F. Codd criticized the then-dominant hierarchical model of databases in his article  A Relational Model of Data for Large Shared Data Banks . Reading the article reveals a striking similarity between hierarchical databases and nested http routes. With PostgREST we attempt to use flexible filtering and embedding rather than nested routes.", 
            "title": "Embracing the Relational Model"
        }, 
        {
            "location": "/#one-thing-well", 
            "text": "PostgREST has a focused scope. It works well with other tools like Nginx. This forces you to cleanly separate the data-centric CRUD operations from other concerns. Use a collection of sharp tools rather than building a big ball of mud.", 
            "title": "One Thing Well"
        }, 
        {
            "location": "/#shared-improvements", 
            "text": "As with any open source project, we all gain from features and fixes in the tool. It's more beneficial than improvements locked inextricably within custom codebases.", 
            "title": "Shared Improvements"
        }, 
        {
            "location": "/#intro-video", 
            "text": "Some things have changed since this video was created but the basics are the same. Learn the big vision behind automating APIs.", 
            "title": "Intro Video"
        }, 
        {
            "location": "/#myths", 
            "text": "", 
            "title": "Myths"
        }, 
        {
            "location": "/#you-have-to-make-tons-of-stored-procs-and-triggers", 
            "text": "Modern PostgreSQL features like auto-updatable views and computed columns make this mostly unnecessary. Triggers do play a part, but generally not for irksome boilerplate. When they are required triggers are preferable to ad-hoc app code anyway, since the former work reliably for any codepath.", 
            "title": "You have to make tons of stored procs and triggers"
        }, 
        {
            "location": "/#exposing-the-database-destroys-encapsulation", 
            "text": "PostgREST does versioning through database schemas. This allows you to expose tables and views without making the app brittle. Underlying tables can be superseded and hidden behind public facing views. The chapter about versioning shows how to do this.", 
            "title": "Exposing the database destroys encapsulation"
        }, 
        {
            "location": "/#conventions", 
            "text": "This guide contains highlighted notes and tangential information interspersed with the text.  \n     Design Consideration \n\n     Contains history which informed the current design. Sometimes it discusses unavoidable tradeoffs or a point of theory.   \n     Invitation to Contribute \n\n     Points out things we know we want to add or improve. They might give you ideas for ways to contribute to the project.   \n     Deprecation Warning \n\n     Alerts you to features which will be removed in the next major (breaking) release.", 
            "title": "Conventions"
        }, 
        {
            "location": "/install/server/", 
            "text": "Installation\n\n\nInstalling from Pre-Built Release\n\n\nThe \nrelease page\n\nhas precompiled binaries for Mac OS X, Windows, and several Linux\ndistros.  Extract the tarball and run the binary inside with no\narguments to see usage instructions:\n\n\n# Untar the release (available at https://github.com/begriffs/postgrest/releases/latest)\n\n$ tar zxf postgrest-[version]-[platform].tar.xz\n\n# Try running it\n$ ./postgrest\n\n# You should see a usage help message\n\n\n\n\n\n    \nInvitation to Contribute\n\n\n    \nI currently build the binaries manually for each architecture.\n    It would be nice to set up an automated build matrix for various\n    architectures. It should support Mac, Windows and 32- and 64-bit\n    versions of\n\n    \nScientific Linux 6\nCentOS\nRHEL 6\n\n\n\n\n\nBuilding from Source\n\n\nWhen a prebuilt binary does not exist for your system you can build\nthe project from source. You'll also need to do this if you want\nto help with development.\n\nStack\n makes it easy.\nIt will install any necessary Haskell dependencies on your system.\n\n\n\n\nInstall Stack\n for your platform\n\n\n\n\n#ubuntu example\n#See the link above for other operating systems\n\nwget -q -O- https://s3.amazonaws.com/download.fpcomplete.com/ubuntu/fpco.key | sudo apt-key add -\necho 'deb http://download.fpcomplete.com/ubuntu/trusty stable main'|sudo tee /etc/apt/sources.list.d/fpco.list\nsudo apt-get update \n sudo apt-get install stack -y\n\n\n\n\n\n\nInstall libpq-dev\n\n\n\n\nsudo apt-get install -y libpq-dev\n\n\n\n\n\n\nBuild \n install in one step\n\n\n\n\ngit clone https://github.com/begriffs/postgrest.git\ncd postgrest\nstack build --install-ghc\nsudo stack install --allow-different-user --local-bin-path /usr/local/bin\n\n\n\n\n\n\nRun the server\n\n\n\n\nIf you want to run the test suite, stack can do that too: \nstack test\n.\n\n\nRunning the Server\n\n\npostgrest postgres://user:pass@host:port/db -a anon_user [other flags]\n\n\n\n\nThe user in the connection string is the \"authenticator role,\" i.e.\na role which is used temporarily to switch into other roles depending\non the authentication request JWT. For simple API's you can use the\nsame role for authenticator and anonymous.\n\n\nThe complete list of options:\n\n\n\n\n-p, --port\n\n\nThe port on which the server will listen for HTTP requests.\n    Defaults to 3000.\n\n\n\n-a, --anonymous (required)\n\n\nThe database role used to execute commands for those requests\n    which provide no JWT authorization.\n\n\n\n-s, --schema\n\n\nThe db schema which you want to expose as an API. For historical\n    reasons it defaults to \n1\n, but you're more likely\n    to want to choose a value of \npublic\n.\n\n\n\n-j, --jwt-secret\n\n\nThe secret passphrase used to encrypt JWT tokens. Defaults to\n    \nsecret\n but do not use the default in production!\n    Load-balanced PostgREST servers should share the same secret.\n\n\n\n-o, --pool\n\n\nMax connections to use in db pool. Defaults to to 10, but you\n    should find an optimal value for your db by running the SQL\n    command \nshow max_connections;\n\n\n\n-m, --max-rows\n\n\nMax number of rows to return in a read request. The default is\n    no limit.\n\n\n\n\n\n\n    \nHiding Password from Process List\n\n\n    \nPassing the database password and JWT secret as naked\n    parameters might not be a good idea because the parameters are\n    visible in a \nps\n listing. One solution is to set\n    environment variables such as PASS and use \n$PASS\n\n    in the connection string.  Another is to use a user-specific\n    \n.pgpass\n\n    file.\n\n\n\n\n\nWhen running \npostgrest\n on the same machine as PostgreSQL, it is also\npossible to connect to the database using the [Unix socket]\n(https://en.wikipedia.org/wiki/Unix_domain_socket) and the\n[Peer Authentication method]\n(http://www.postgresql.org/docs/current/static/auth-methods.html#AUTH-PEER)\nas an alternative to TCP/IP communication and authentication with a password.\n\n\nThe Peer Authentication grants access to the database to any Unix user\nwho connects as a user of the same name in the database.\nSince the empty host resolves to the Unix socket]\n(http://www.postgresql.org/docs/current/static/libpq-connect.html#AEN42494)\nand the password can be omitted in this case,\nthe command line is reduced to:\n\n\nsudo -u user postgrest postgres://user@/db [flags]\n\n\n\n\nwhere the \nsudo -u user\n command runs the following command as given \nuser\n.\n\n\nIf you create a Unix user \npostgrest\n and a database user \npostgrest\n\nfor example, the command becomes:\n\n\nsudo -u postgrest postgrest postgres://postgrest@/db [flags]\n\n\n\n\nThe first \npostgrest\n is the Unix user name, the second \npostgrest\n\nis the name of the executable, the third \npostgrest\n is the name\nof the database user.\n\n\nInstall via Homebrew (Mac OS X)\n\n\nYou can use the Homebrew package manager to install PostgREST on Mac\n\n\n# Ensure brew is up to date\nbrew update\n\n# Check for any problems with brew's setup\nbrew doctor\n\n# Install the postgrest package\nbrew install postgrest\n\n\n\n\nThis will automatically install PostgreSQL as a dependency (see the \nInstalling PostgreSQL\n section for setup instructions). The process tends to take up to 15 minutes to install the package and its dependencies.\n\n\nAfter installation completes, the tool is added to your $PATH and can be used from anywhere with:\n\n\npostgrest --help\n\n\n\n\nInstalling PostgreSQL\n\n\nTo use PostgREST you will need an underlying database (PostgreSQL version 9.3 or greater is required). You can use something like Amazon \nRDS\n but installing your own locally is cheaper and more convenient for development.\n\n\n\n\nInstructions for OS X\n\n\nInstructions for Ubuntu 14.04\n\n\nInstaller for Windows", 
            "title": "The Server"
        }, 
        {
            "location": "/install/server/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/install/server/#installing-from-pre-built-release", 
            "text": "The  release page \nhas precompiled binaries for Mac OS X, Windows, and several Linux\ndistros.  Extract the tarball and run the binary inside with no\narguments to see usage instructions:  # Untar the release (available at https://github.com/begriffs/postgrest/releases/latest)\n\n$ tar zxf postgrest-[version]-[platform].tar.xz\n\n# Try running it\n$ ./postgrest\n\n# You should see a usage help message  \n     Invitation to Contribute \n\n     I currently build the binaries manually for each architecture.\n    It would be nice to set up an automated build matrix for various\n    architectures. It should support Mac, Windows and 32- and 64-bit\n    versions of\n\n     Scientific Linux 6 CentOS RHEL 6", 
            "title": "Installing from Pre-Built Release"
        }, 
        {
            "location": "/install/server/#building-from-source", 
            "text": "When a prebuilt binary does not exist for your system you can build\nthe project from source. You'll also need to do this if you want\nto help with development. Stack  makes it easy.\nIt will install any necessary Haskell dependencies on your system.   Install Stack  for your platform   #ubuntu example\n#See the link above for other operating systems\n\nwget -q -O- https://s3.amazonaws.com/download.fpcomplete.com/ubuntu/fpco.key | sudo apt-key add -\necho 'deb http://download.fpcomplete.com/ubuntu/trusty stable main'|sudo tee /etc/apt/sources.list.d/fpco.list\nsudo apt-get update   sudo apt-get install stack -y   Install libpq-dev   sudo apt-get install -y libpq-dev   Build   install in one step   git clone https://github.com/begriffs/postgrest.git\ncd postgrest\nstack build --install-ghc\nsudo stack install --allow-different-user --local-bin-path /usr/local/bin   Run the server   If you want to run the test suite, stack can do that too:  stack test .", 
            "title": "Building from Source"
        }, 
        {
            "location": "/install/server/#running-the-server", 
            "text": "postgrest postgres://user:pass@host:port/db -a anon_user [other flags]  The user in the connection string is the \"authenticator role,\" i.e.\na role which is used temporarily to switch into other roles depending\non the authentication request JWT. For simple API's you can use the\nsame role for authenticator and anonymous.  The complete list of options:   -p, --port  The port on which the server will listen for HTTP requests.\n    Defaults to 3000.  -a, --anonymous (required)  The database role used to execute commands for those requests\n    which provide no JWT authorization.  -s, --schema  The db schema which you want to expose as an API. For historical\n    reasons it defaults to  1 , but you're more likely\n    to want to choose a value of  public .  -j, --jwt-secret  The secret passphrase used to encrypt JWT tokens. Defaults to\n     secret  but do not use the default in production!\n    Load-balanced PostgREST servers should share the same secret.  -o, --pool  Max connections to use in db pool. Defaults to to 10, but you\n    should find an optimal value for your db by running the SQL\n    command  show max_connections;  -m, --max-rows  Max number of rows to return in a read request. The default is\n    no limit.   \n     Hiding Password from Process List \n\n     Passing the database password and JWT secret as naked\n    parameters might not be a good idea because the parameters are\n    visible in a  ps  listing. One solution is to set\n    environment variables such as PASS and use  $PASS \n    in the connection string.  Another is to use a user-specific\n     .pgpass \n    file.   When running  postgrest  on the same machine as PostgreSQL, it is also\npossible to connect to the database using the [Unix socket]\n(https://en.wikipedia.org/wiki/Unix_domain_socket) and the\n[Peer Authentication method]\n(http://www.postgresql.org/docs/current/static/auth-methods.html#AUTH-PEER)\nas an alternative to TCP/IP communication and authentication with a password.  The Peer Authentication grants access to the database to any Unix user\nwho connects as a user of the same name in the database.\nSince the empty host resolves to the Unix socket]\n(http://www.postgresql.org/docs/current/static/libpq-connect.html#AEN42494)\nand the password can be omitted in this case,\nthe command line is reduced to:  sudo -u user postgrest postgres://user@/db [flags]  where the  sudo -u user  command runs the following command as given  user .  If you create a Unix user  postgrest  and a database user  postgrest \nfor example, the command becomes:  sudo -u postgrest postgrest postgres://postgrest@/db [flags]  The first  postgrest  is the Unix user name, the second  postgrest \nis the name of the executable, the third  postgrest  is the name\nof the database user.", 
            "title": "Running the Server"
        }, 
        {
            "location": "/install/server/#install-via-homebrew-mac-os-x", 
            "text": "You can use the Homebrew package manager to install PostgREST on Mac  # Ensure brew is up to date\nbrew update\n\n# Check for any problems with brew's setup\nbrew doctor\n\n# Install the postgrest package\nbrew install postgrest  This will automatically install PostgreSQL as a dependency (see the  Installing PostgreSQL  section for setup instructions). The process tends to take up to 15 minutes to install the package and its dependencies.  After installation completes, the tool is added to your $PATH and can be used from anywhere with:  postgrest --help", 
            "title": "Install via Homebrew (Mac OS X)"
        }, 
        {
            "location": "/install/server/#installing-postgresql", 
            "text": "To use PostgREST you will need an underlying database (PostgreSQL version 9.3 or greater is required). You can use something like Amazon  RDS  but installing your own locally is cheaper and more convenient for development.   Instructions for OS X  Instructions for Ubuntu 14.04  Installer for Windows", 
            "title": "Installing PostgreSQL"
        }, 
        {
            "location": "/install/ecosystem/", 
            "text": "Ecosystem\n\n\nClient-Side Libraries\n\n\n\n\ncalebmer/postgrest-client\n - Advanced JS client for the PostgREST API\n\n\nmithril.postgrest\n - Mithril plugin to create and authenticate requests\n\n\nlewisjared/postgrest-request\n - node interface to postgrest instances\n\n\nJarvusInnovations/jarvus-postgrest-apikit\n - Sencha framework package for binding models/stores/proxies to PostgREST tables\n\n\ndavidthewatson/postgrest_python_requests_client\n - python client featuring JWT auth and pagination of result sets\n\n\n\n\nExtensions\n\n\n\n\nsrid/spas\n - allow file uploads and basic auth\n\n\n\n\nExample Apps\n\n\n\n\nruslantalpa/blogdemo\n - blog api demo in a vagrant image\n\n\ntimwis/ext-postgrest-crud\n - browser-based spreadsheet\n\n\nsrid/chronicle\n - tracking a tree of personal memories\n\n\nbegriffs/postgrest-example\n - how to configure a db for use as an API\n\n\nmarmelab/ng-admin-postgrest\n - automatic database admin panel\n\n\ntyrchen/goodfilm\n - example film api\n\n\n\n\nIn Production\n\n\n\n\nCatarse", 
            "title": "Ecosystem"
        }, 
        {
            "location": "/install/ecosystem/#ecosystem", 
            "text": "", 
            "title": "Ecosystem"
        }, 
        {
            "location": "/install/ecosystem/#client-side-libraries", 
            "text": "calebmer/postgrest-client  - Advanced JS client for the PostgREST API  mithril.postgrest  - Mithril plugin to create and authenticate requests  lewisjared/postgrest-request  - node interface to postgrest instances  JarvusInnovations/jarvus-postgrest-apikit  - Sencha framework package for binding models/stores/proxies to PostgREST tables  davidthewatson/postgrest_python_requests_client  - python client featuring JWT auth and pagination of result sets", 
            "title": "Client-Side Libraries"
        }, 
        {
            "location": "/install/ecosystem/#extensions", 
            "text": "srid/spas  - allow file uploads and basic auth", 
            "title": "Extensions"
        }, 
        {
            "location": "/install/ecosystem/#example-apps", 
            "text": "ruslantalpa/blogdemo  - blog api demo in a vagrant image  timwis/ext-postgrest-crud  - browser-based spreadsheet  srid/chronicle  - tracking a tree of personal memories  begriffs/postgrest-example  - how to configure a db for use as an API  marmelab/ng-admin-postgrest  - automatic database admin panel  tyrchen/goodfilm  - example film api", 
            "title": "Example Apps"
        }, 
        {
            "location": "/install/ecosystem/#in-production", 
            "text": "Catarse", 
            "title": "In Production"
        }, 
        {
            "location": "/api/reading/", 
            "text": "Requesting Information\n\n\nTables and Views\n\n\n\n\n\u2705 Cacheable, prefetchable\n\n\n\u2705 Idempotent\n\n\n\n\nThe list of accessible tables and views is provided at\n\n\nGET /\n\n\n\n\nEvery view and table accessible by the active db role is exposed\nin a one-level deep route. For instance the full contents of a table\n\npeople\n is returned at\n\n\nGET /people\n\n\n\n\nThere are no \ndeeply/nested/routes\n. Each route provides \nOPTIONS\n,\n\nGET\n, \nPOST\n, \nPATCH\n, and \nDELETE\n verbs depending entirely\non database permissions.\n\n\n\n  \nDesign Consideration\n\n\n  \nWhy not provide nested routes? Many APIs allow nesting to\n  retrieve related information, such as \n/films/1/director\n.\n  We offer a more flexible mechanism (inspired by GraphQL) to embed\n  related information. It can handle one-to-many and many-to-many\n  relationships. This is covered in the section about Embedding.\n\n\n\n\n\nStored Procedures\n\n\n\n\n\u274c Cannot necessarily be cached or prefetched\n\n\n\u274c Not necessarily idempotent\n\n\n\n\nEvery stored procedure is accessible under the \n/rpc\n prefix. The\nAPI endpoint supports only POST which executes the function.\n\n\nPOST /rpc/proc_name\n\n\n\n\nPostgREST supports calling procedures with \nnamed\narguments\n.\nInclude a JSON object in the request payload and each\nkey/value of the object will become an argument.\n\n\n\n  \nDesign Consideration\n\n\n  \nWhy the /rpc prefix? One reason is to avoid name collisions\n  between views and procedures. It also helps emphasize to API\n  consumers that these functions are not normal restful things.\n  The functions can have arbitrary and surprising behavior, not\n  the standard \"post creates a resource\" thing that users expect\n  from the other routes.\n\n\n  \nWe considered allowing GET requests for functions that are\n  marked non-volatile but could not reconcile how to pass in\n  parameters. Query string arguments are reserved for shaping/filtering\n  the output, not providing input.\n\n\n\n\n\nFiltering\n\n\nFiltering Rows\n\n\nYou can filter result rows by adding conditions on columns, each\ncondition a query string parameter.  For instance, to return people\naged under 13 years old:\n\n\nGET /people?age=lt.13\n\n\n\n\nAdding multiple parameters conjoins the conditions:\n\n\nGET /people?age=gte.18\nstudent=is.true\n\n\n\n\nThese operators are available:\n\n\n\n\n\n\n\n\nabbreviation\n\n\nmeaning\n\n\n\n\n\n\n\n\n\n\neq\n\n\nequals\n\n\n\n\n\n\ngte\n\n\ngreater than or equal\n\n\n\n\n\n\ngt\n\n\ngreater than\n\n\n\n\n\n\nlte\n\n\nless than or equal\n\n\n\n\n\n\nlt\n\n\nless than\n\n\n\n\n\n\nneq\n\n\nnot equal\n\n\n\n\n\n\nlike\n\n\nLIKE operator (use * in place of %)\n\n\n\n\n\n\nilike\n\n\nILIKE operator (use * in place of %)\n\n\n\n\n\n\nin\n\n\none of a list of values e.g. \n?a=in.1,2,3\n\n\n\n\n\n\nnotin\n\n\nnot one of a list of values e.g. \n?a=notin.1,2,3\n\n\n\n\n\n\nis\n\n\nchecking for exact equality (null,true,false)\n\n\n\n\n\n\nisnot\n\n\nchecking for exact inequality (null,true,false)\n\n\n\n\n\n\n@@\n\n\nfull-text search using to_tsquery\n\n\n\n\n\n\n@\n\n\ncontains e.g. \n?tags=@\n.{example, new}\n\n\n\n\n\n\n@\n\n\ncontained in e.g. \nvalues=\n@{1,2,3}\n\n\n\n\n\n\nnot\n\n\nnegates another operator, see below\n\n\n\n\n\n\n\n\nTo negate any operator, prefix it with \nnot\n like \n?a=not.eq.2\n.\n\n\nFor more complicated filters (such as those involving condition 1\n\nOR\n condition 2) you will have to create a new view in the database.\n\n\nFilters may be applied to \ncomputed\ncolumns\n\nas well as actual table/view columns, even though the computed\ncolumns will not appear in the output.\n\n\nFiltering Columns\n\n\nYou can customize which columns are returned by using the \nselect\n\nparameter:\n\n\nGET /people?select=age,height,weight\n\n\n\n\nTo cast the column types, add a double colon\n\n\nGET /people?select=age::text,height,weight\n\n\n\n\nNot all type coercions are possible, and you will get an error\ndescribing any problems from selection or type casting.\n\n\nThe \nselect\n keyword is reserved. You thus cannot filter rows based\non a column named select. Then again it is a reserved SQL keyword\ntoo, hence an unlikely column name.\n\n\nInside JSONB\n\n\nPostgreSQL \n=9.4.2 supports native JSON columns and can even index\nthem by internal keys using the \njsonb\n column type. PostgREST\nallows you to filter results by internal JSON object values. Use\nthe single- and double-arrows to path into and obtain values, e.g.\n\n\nGET /stuff?json_col-\na-\nb=eq.2\n\n\n\n\nThis query finds rows in \nstuff\n where \njson_col-\n'a'-\n'b'\n is\nequal to 2 (or \"2\" -- it coerces as needed). The final arrow must\nbe the double kind, \n-\n, or else PostgREST will not attempt to\nlook inside the JSON.\n\n\nOrdering\n\n\nThe reserved word \norder\n reorders the response rows.  It uses a\ncomma-separated list of columns and directions:\n\n\nGET /people?order=age.desc,height.asc\n\n\n\n\nIf no direction is specified it defaults to ascending order:\n\n\nGET /people?order=age\n\n\n\n\nIf you care where nulls are sorted, add \nnullsfirst\n or \nnullslast\n:\n\n\nGET /people?order=age.nullsfirst\nGET /people?order=age.desc.nullslast\n\n\n\n\nTo order the embedded items, you need to specify the tree path for the order param like so.\n\n\nGET /projects?select=id,name,tasks{id,name}\norder=id.asc\ntasks.order=name.asc\n\n\n\n\nYou can also use \ncomputed\ncolumns\n\nto order the results, even though the computed\ncolumns will not appear in the output.\n\n\nLimiting and Pagination\n\n\nPagination by Limit-Offset\n\n\nPostgREST uses HTTP range headers for limiting and describing the\nsize of results. Every response contains the current range and total\nresults:\n\n\nRange-Unit: items\nContent-Range \u2192 0-14/15\n\n\n\n\nThis means items zero through fourteen are returned out of a total\nof fifteen -- i.e. all of them. This information is available in\nevery response and can help you render pagination controls on the\nclient. This is a RFC7233-compliant solution that keeps the response\nJSON cleaner.\n\n\nThe client can set the limit and offset of a request by setting the\n\nRange\n header. Translate the limit and offset into a range. To\nrequest the first five elements, include these request headers:\n\n\nRange-Unit: items\nRange: 0-4\n\n\n\n\nYou can also use open-ended ranges for an offset with no limit:\n\nRange: 10-\n.\n\n\nIn addition to the \nRange\n header, you can use \nlimit\n and \noffset\n parameters\nto achieve the same result.\n\n\nYou can also set a limit (but not offset) for the embedded items like so\n\n\n/posts?select=id,title,body,comments{id,email,body}\nlimit=10\ncomments.limit=3\n\n\n\n\nThe above request will return the first 10 posts and for each of the posts, 3 comments at most\n\n\nSuppressing Counts\n\n\nSometimes knowing the total row count of a query is unnecessary and\nonly adds extra cost to the database query. So you can skip the\ncount total using a \nPrefer\n header as:\n\n\nPrefer: count=none\n\n\n\n\nWith count suppressed the PostgREST response will look like:\n\n\nRange-Unit: items\nContent-Range \u2192 0-14/*\n\n\n\n\nEmbedding Foreign Entities\n\n\nTo help you make fewer requests, PostgREST allows the embedding of\ntraditional SQL relationships into a response. Suppose you have a\n\nprojects\n table which references \nclients\n through a foreign key\ncalled \nclient_id\n. When listing projects through the API you can\nhave it embed the client within each project response. For example,\n\n\nGET /projects?id=eq.1\nselect=id, name, clients{*}\n\n\n\n\nNotice this is the same \nselect\n keyword which is used to choose\nwhich columns to include. When a column name is followed by parentheses\nthat means to fetch the entire record and nest it. You include a\nlist of columns inside the parens, or asterisk to request all\ncolumns.\n\n\nThe embedding works for 1-N, N-1, and N-N relationships. That means\nyou could also ask for a client and all their projects:\n\n\nGET /clients?id=eq.42\nselect=id, name, projects{*}\n\n\n\n\nIn the examples above we asked for all columns in the embedded resource\nbut the the select query is recursive. You could for instance specify\n\n\nGET /foo?select=x, y, bar{z, w, baz{*}}\n\n\n\n\nYou can select not only using table names, but also foreign key column names!\nThis is especially needed when you have a table with two foreign keys pointing to the same table, for example billing_address_id and shipping_address_id.\nTo embed the same foreign key row from our client example earlier\nyou could do the following:\n\n\nGET /projects?id=eq.1\nselect=id, name, client_id{*}\n\n\n\n\nIn the response there will be a \nclient_id\n object containing all\nthe data for that row.\n\n\nHowever, a \nclient_id\n object doesn't make a lot of sense, so you\ncould do one of two things. Tell PostgREST that you want the key renamed by using the \nalias\n feature like so \nclient:client_id{*}\n, or just try \nclient{*}\n\nin the select parameter! PostgREST supports smart ducktype checking\nfor common foreign key names, so if your column name ends with\n\n_id\n, \n_fk\n, or any variation of the two (including camelcase)\nyou can embed a row with just the name's beginning.\n\n\nSo for a complete example:\n\n\nGET /projects?id=eq.1\nselect=id, name, client{*}\n\n\n\n\nWould embed in the \nclient\n key the row referenced with \nclient_id\n.\n\n\nThe \nalias\n feature works for embedded entities and also for regular columns. This is useful in situations where for example you use different naming conventions in the database and frontend.\n\n\nThe following request will produce the output below:\n\n\nGET /orders?id=eq.1\nselect=orderId:id, customer:customer_id{customerId:id, customerName:name}\n\n\n\n\n[\n  {\n    \norderId\n: 1,\n    \ncustomer\n: {\n      \ncustomerId\n: 1,\n      \ncustomerName\n: \nJohn Smith\n\n    }\n  }\n]\n\n\n\n\nIf you want to apply filters to the embedded items, you can do that like so:\n\n\nGET /clients?id=eq.42\nselect=id,name,projects{id,name,is_active}\nprojects.is_active=eq.true\n\n\n\n\nThe above request will return the client with id=42 and all the projects for that client that are still active\n\n\n\n    \nDesign Consideration\n\n    \nIn order for this feature to work as expected after a schema change, PostgREST currently requires to be restarted.\n\n\n\n\n\nResponse Format\n\n\nQuery responses default to JSON but you can get them in CSV as well. Just make your request with the header\n\n\nAccept: text/csv\n\n\n\n\nSingular vs Plural\n\n\nMany APIs distinguish plural and singular resources, e.g.\n/stories\n\nvs \n/stories/1\n. Why do we use \n/stories?id=eq.1\n? It is because a\nsingle resource is for us a row determined by a primary key, and\nprimary keys can be \ncompound\n (meaning defined across more than\none column). The common urls come from a degenerate case of simple\n(and overwhelmingly numeric) primary keys often introduced automatically\nbe Object Relational Mapping.\n\n\nFor consistency's sake all these endpoints return a JSON array,\n\n/stories\n, \n/stories?genre=eq.mystery\n, \n/stories?id=eq.1\n. They\nare all filtering a bigger array. However you might want the\nlast one to return a single JSON object, not an array with one\nelement. To request a singular response send the header\n\nPrefer: plurality=singular\n.\n\n\nData Schema\n\n\nAs well as issuing a \nGET /\n to obtain a list of the tables, views,\nand stored procedures available, you can get more information about\nany particular endpoint.\n\n\nOPTIONS /my_view\n\n\n\n\nThis will include the row names, their types, primary key\ninformation, and foreign keys for the given table or view.\n\n\n\n    \nSchema Changes\n\n\n    \nNote that when the schema of your database changes PostgREST will not reflect\n    the change. You have to either restart PostgREST or send its running process\n    a HUP signal:\n\n    \nkillall -HUP postgrest\n\n\n\n\n\nCORS\n\n\nPostgREST sets highly permissive cross origin resource sharing.  It\naccepts Ajax requests from any domain.", 
            "title": "Reading"
        }, 
        {
            "location": "/api/reading/#requesting-information", 
            "text": "", 
            "title": "Requesting Information"
        }, 
        {
            "location": "/api/reading/#tables-and-views", 
            "text": "\u2705 Cacheable, prefetchable  \u2705 Idempotent   The list of accessible tables and views is provided at  GET /  Every view and table accessible by the active db role is exposed\nin a one-level deep route. For instance the full contents of a table people  is returned at  GET /people  There are no  deeply/nested/routes . Each route provides  OPTIONS , GET ,  POST ,  PATCH , and  DELETE  verbs depending entirely\non database permissions.  \n   Design Consideration \n\n   Why not provide nested routes? Many APIs allow nesting to\n  retrieve related information, such as  /films/1/director .\n  We offer a more flexible mechanism (inspired by GraphQL) to embed\n  related information. It can handle one-to-many and many-to-many\n  relationships. This is covered in the section about Embedding.", 
            "title": "Tables and Views"
        }, 
        {
            "location": "/api/reading/#stored-procedures", 
            "text": "\u274c Cannot necessarily be cached or prefetched  \u274c Not necessarily idempotent   Every stored procedure is accessible under the  /rpc  prefix. The\nAPI endpoint supports only POST which executes the function.  POST /rpc/proc_name  PostgREST supports calling procedures with  named\narguments .\nInclude a JSON object in the request payload and each\nkey/value of the object will become an argument.  \n   Design Consideration \n\n   Why the /rpc prefix? One reason is to avoid name collisions\n  between views and procedures. It also helps emphasize to API\n  consumers that these functions are not normal restful things.\n  The functions can have arbitrary and surprising behavior, not\n  the standard \"post creates a resource\" thing that users expect\n  from the other routes. \n\n   We considered allowing GET requests for functions that are\n  marked non-volatile but could not reconcile how to pass in\n  parameters. Query string arguments are reserved for shaping/filtering\n  the output, not providing input.", 
            "title": "Stored Procedures"
        }, 
        {
            "location": "/api/reading/#filtering", 
            "text": "", 
            "title": "Filtering"
        }, 
        {
            "location": "/api/reading/#filtering-rows", 
            "text": "You can filter result rows by adding conditions on columns, each\ncondition a query string parameter.  For instance, to return people\naged under 13 years old:  GET /people?age=lt.13  Adding multiple parameters conjoins the conditions:  GET /people?age=gte.18 student=is.true  These operators are available:     abbreviation  meaning      eq  equals    gte  greater than or equal    gt  greater than    lte  less than or equal    lt  less than    neq  not equal    like  LIKE operator (use * in place of %)    ilike  ILIKE operator (use * in place of %)    in  one of a list of values e.g.  ?a=in.1,2,3    notin  not one of a list of values e.g.  ?a=notin.1,2,3    is  checking for exact equality (null,true,false)    isnot  checking for exact inequality (null,true,false)    @@  full-text search using to_tsquery    @  contains e.g.  ?tags=@ .{example, new}    @  contained in e.g.  values= @{1,2,3}    not  negates another operator, see below     To negate any operator, prefix it with  not  like  ?a=not.eq.2 .  For more complicated filters (such as those involving condition 1 OR  condition 2) you will have to create a new view in the database.  Filters may be applied to  computed\ncolumns \nas well as actual table/view columns, even though the computed\ncolumns will not appear in the output.", 
            "title": "Filtering Rows"
        }, 
        {
            "location": "/api/reading/#filtering-columns", 
            "text": "You can customize which columns are returned by using the  select \nparameter:  GET /people?select=age,height,weight  To cast the column types, add a double colon  GET /people?select=age::text,height,weight  Not all type coercions are possible, and you will get an error\ndescribing any problems from selection or type casting.  The  select  keyword is reserved. You thus cannot filter rows based\non a column named select. Then again it is a reserved SQL keyword\ntoo, hence an unlikely column name.", 
            "title": "Filtering Columns"
        }, 
        {
            "location": "/api/reading/#inside-jsonb", 
            "text": "PostgreSQL  =9.4.2 supports native JSON columns and can even index\nthem by internal keys using the  jsonb  column type. PostgREST\nallows you to filter results by internal JSON object values. Use\nthe single- and double-arrows to path into and obtain values, e.g.  GET /stuff?json_col- a- b=eq.2  This query finds rows in  stuff  where  json_col- 'a'- 'b'  is\nequal to 2 (or \"2\" -- it coerces as needed). The final arrow must\nbe the double kind,  - , or else PostgREST will not attempt to\nlook inside the JSON.", 
            "title": "Inside JSONB"
        }, 
        {
            "location": "/api/reading/#ordering", 
            "text": "The reserved word  order  reorders the response rows.  It uses a\ncomma-separated list of columns and directions:  GET /people?order=age.desc,height.asc  If no direction is specified it defaults to ascending order:  GET /people?order=age  If you care where nulls are sorted, add  nullsfirst  or  nullslast :  GET /people?order=age.nullsfirst\nGET /people?order=age.desc.nullslast  To order the embedded items, you need to specify the tree path for the order param like so.  GET /projects?select=id,name,tasks{id,name} order=id.asc tasks.order=name.asc  You can also use  computed\ncolumns \nto order the results, even though the computed\ncolumns will not appear in the output.", 
            "title": "Ordering"
        }, 
        {
            "location": "/api/reading/#limiting-and-pagination", 
            "text": "", 
            "title": "Limiting and Pagination"
        }, 
        {
            "location": "/api/reading/#pagination-by-limit-offset", 
            "text": "PostgREST uses HTTP range headers for limiting and describing the\nsize of results. Every response contains the current range and total\nresults:  Range-Unit: items\nContent-Range \u2192 0-14/15  This means items zero through fourteen are returned out of a total\nof fifteen -- i.e. all of them. This information is available in\nevery response and can help you render pagination controls on the\nclient. This is a RFC7233-compliant solution that keeps the response\nJSON cleaner.  The client can set the limit and offset of a request by setting the Range  header. Translate the limit and offset into a range. To\nrequest the first five elements, include these request headers:  Range-Unit: items\nRange: 0-4  You can also use open-ended ranges for an offset with no limit: Range: 10- .  In addition to the  Range  header, you can use  limit  and  offset  parameters\nto achieve the same result.  You can also set a limit (but not offset) for the embedded items like so  /posts?select=id,title,body,comments{id,email,body} limit=10 comments.limit=3  The above request will return the first 10 posts and for each of the posts, 3 comments at most", 
            "title": "Pagination by Limit-Offset"
        }, 
        {
            "location": "/api/reading/#suppressing-counts", 
            "text": "Sometimes knowing the total row count of a query is unnecessary and\nonly adds extra cost to the database query. So you can skip the\ncount total using a  Prefer  header as:  Prefer: count=none  With count suppressed the PostgREST response will look like:  Range-Unit: items\nContent-Range \u2192 0-14/*", 
            "title": "Suppressing Counts"
        }, 
        {
            "location": "/api/reading/#embedding-foreign-entities", 
            "text": "To help you make fewer requests, PostgREST allows the embedding of\ntraditional SQL relationships into a response. Suppose you have a projects  table which references  clients  through a foreign key\ncalled  client_id . When listing projects through the API you can\nhave it embed the client within each project response. For example,  GET /projects?id=eq.1 select=id, name, clients{*}  Notice this is the same  select  keyword which is used to choose\nwhich columns to include. When a column name is followed by parentheses\nthat means to fetch the entire record and nest it. You include a\nlist of columns inside the parens, or asterisk to request all\ncolumns.  The embedding works for 1-N, N-1, and N-N relationships. That means\nyou could also ask for a client and all their projects:  GET /clients?id=eq.42 select=id, name, projects{*}  In the examples above we asked for all columns in the embedded resource\nbut the the select query is recursive. You could for instance specify  GET /foo?select=x, y, bar{z, w, baz{*}}  You can select not only using table names, but also foreign key column names!\nThis is especially needed when you have a table with two foreign keys pointing to the same table, for example billing_address_id and shipping_address_id.\nTo embed the same foreign key row from our client example earlier\nyou could do the following:  GET /projects?id=eq.1 select=id, name, client_id{*}  In the response there will be a  client_id  object containing all\nthe data for that row.  However, a  client_id  object doesn't make a lot of sense, so you\ncould do one of two things. Tell PostgREST that you want the key renamed by using the  alias  feature like so  client:client_id{*} , or just try  client{*} \nin the select parameter! PostgREST supports smart ducktype checking\nfor common foreign key names, so if your column name ends with _id ,  _fk , or any variation of the two (including camelcase)\nyou can embed a row with just the name's beginning.  So for a complete example:  GET /projects?id=eq.1 select=id, name, client{*}  Would embed in the  client  key the row referenced with  client_id .  The  alias  feature works for embedded entities and also for regular columns. This is useful in situations where for example you use different naming conventions in the database and frontend.  The following request will produce the output below:  GET /orders?id=eq.1 select=orderId:id, customer:customer_id{customerId:id, customerName:name}  [\n  {\n     orderId : 1,\n     customer : {\n       customerId : 1,\n       customerName :  John Smith \n    }\n  }\n]  If you want to apply filters to the embedded items, you can do that like so:  GET /clients?id=eq.42 select=id,name,projects{id,name,is_active} projects.is_active=eq.true  The above request will return the client with id=42 and all the projects for that client that are still active  \n     Design Consideration \n     In order for this feature to work as expected after a schema change, PostgREST currently requires to be restarted.", 
            "title": "Embedding Foreign Entities"
        }, 
        {
            "location": "/api/reading/#response-format", 
            "text": "Query responses default to JSON but you can get them in CSV as well. Just make your request with the header  Accept: text/csv", 
            "title": "Response Format"
        }, 
        {
            "location": "/api/reading/#singular-vs-plural", 
            "text": "Many APIs distinguish plural and singular resources, e.g. /stories \nvs  /stories/1 . Why do we use  /stories?id=eq.1 ? It is because a\nsingle resource is for us a row determined by a primary key, and\nprimary keys can be  compound  (meaning defined across more than\none column). The common urls come from a degenerate case of simple\n(and overwhelmingly numeric) primary keys often introduced automatically\nbe Object Relational Mapping.  For consistency's sake all these endpoints return a JSON array, /stories ,  /stories?genre=eq.mystery ,  /stories?id=eq.1 . They\nare all filtering a bigger array. However you might want the\nlast one to return a single JSON object, not an array with one\nelement. To request a singular response send the header Prefer: plurality=singular .", 
            "title": "Singular vs Plural"
        }, 
        {
            "location": "/api/reading/#data-schema", 
            "text": "As well as issuing a  GET /  to obtain a list of the tables, views,\nand stored procedures available, you can get more information about\nany particular endpoint.  OPTIONS /my_view  This will include the row names, their types, primary key\ninformation, and foreign keys for the given table or view.  \n     Schema Changes \n\n     Note that when the schema of your database changes PostgREST will not reflect\n    the change. You have to either restart PostgREST or send its running process\n    a HUP signal:\n\n     killall -HUP postgrest", 
            "title": "Data Schema"
        }, 
        {
            "location": "/api/reading/#cors", 
            "text": "PostgREST sets highly permissive cross origin resource sharing.  It\naccepts Ajax requests from any domain.", 
            "title": "CORS"
        }, 
        {
            "location": "/api/writing/", 
            "text": "Updating Data\n\n\nRecord Creation\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nTo create a row in a database table post a JSON object whose keys\nare the names of the columns you would like to create. Missing keys\nwill be set to default values when applicable.\n\n\nPOST /table_name\n{ \ncol1\n: \nvalue1\n, \ncol2\n: \nvalue2\n }\n\n\n\n\nThe response will include a \nLocation\n header describing where to\nfind the new object. If you would like to get the full object back\nin the response to your request, include the header \nPrefer:\nreturn=representation\n. That way you won't have to make another\nHTTP call to discover properties that may have been filled in on\nthe server side.\n\n\nBulk Insertion\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nYou can POST a JSON array or CSV to insert multiple rows in a single\nHTTP request. Note that using CSV requires less parsing on the server\nand is \nmuch faster\n.\n\n\nExample of CSV bulk insert. Simply post to a table route with\n\nContent-Type: text/csv\n and include the names of the columns as\nthe first row. For instance\n\n\nPOST /people\nname,age,height\nJ Doe,62,70\nJonas,10,55\n\n\n\n\nAn empty field (\n,,\n) is coerced to an empty string and the reserved\nword \nNULL\n is mapped to the SQL null value. Note that there should\nbe no spaces between the column names and commas.\n\n\nExample of JSON bulk insert. Send an array:\n\n\nPOST /people\n[\n  { \nname\n: \nJ Doe\n, \nage\n: 62, \nheight\n: 70 },\n  { \nname\n: \nJanus\n, \nage\n: 10, \nheight\n: 55 }\n]\n\n\n\n\nIf you would like to get the full object back in the response to\nyour request, include the header \nPrefer: return=representation\n.\nChances are you only want certain information back, though, like\ncreated ids. You can pass a \nselect\n parameter to affect the shape\nof the response (further documented in the \nreading\n\npage). For instance\n\n\nPOST /people?select=id\n[...]\n\n\n\n\nreturns something like\n\n\n[ { \nid\n: 1 }, { \nid\n: 2 } ]\n\n\n\n\nMultiple Tables Insertion or Update\n\n\nThe cleanest way to insert or update data into multiple tables using only one POST/PATCH request\nis to create a view that will join all target tables and present a single endpoint.\nIn our example let's assume one users table and one companies table.\nIn this case, we want a signup endpoint to create the first user within a company.\nAnd for this endpoint we want to insert with one request both user and company.\n\n\nCREATE TABLE companies (\n    id serial primary key,\n    name text unique\n);\n\nCREATE TABLE users (\n    id serial primary key,\n    name text not null,\n    pass text,\n    company_id integer not null references companies\n);\n\n\n\n\nHaving both tables created we create a view that joins them to be used\nas a \n/signup\n endpoint.\n\n\nCREATE VIEW signup AS\n    SELECT\n        c.name AS company_name,\n        u.name AS user_name,\n        u.pass\n    FROM\n        public.users u\n        JOIN public.companies c ON c.id = u.company_id;\n\n\n\n\n\nAfter the signup view creation, we can issue \nGET\n requests to read data\nfrom users and companies, but any atempt to \nPOST\n or \nPATCH\n data will fail.\nPostgreSQL won't allow any data change on views that have a \nJOIN\n \nclause in their \nFROM\n without a proper \nINSTEAD OF\n trigger.\nSo in the example bellow we create a trigger to allow insertion of data in the signup view.\nThe trigger is a simple PL/pgSQL function that first inserts into the companies table and\nuses the newly create company_id to create its first user.\n\n\nCREATE FUNCTION signup()\nRETURNS trigger\nLANGUAGE plpgsql\nAS $$\nDECLARE\n  vcompany_id int;\nBEGIN\n  INSERT INTO companies (name) VALUES (new.company_name) RETURNING id INTO vcompany_id;\n  INSERT INTO users (name, pass, company_id) VALUES (new.user_name, new.pass, vcompany_id);\nRETURN new;\nEND;\n$$;\n\nCREATE TRIGGER signup\nINSTEAD OF INSERT ON signup\nFOR EACH ROW\nEXECUTE PROCEDURE signup();\n\n\n\n\nAfter the trigger creation we can issue a normal \nPOST\n request to our signup endpoint:\n\n\nPOST /signup\n{ \ncompany_name\n: \nfoo\n, \nuser_name\n: \nbar\n }\n\n\n\n\nFor an endpoint such as signup its usually not desirable to have a \nPATCH\n route for updates,\nand we will skip this example for the sake of brevity. But it would be implemented in a very\nsimilar way to our \nPOST\n example.\n\n\n\n    \nDesign Consideration\n\n\n    \nIt's advisable to create a separate trigger for \nUPDATE\n and \nINSERT\n\n    avoiding conditionals that decide which is the trigger current operation.\n    This makes it easier to change code for (or even disable) one operation without interfering with others while\n    improving readability.\n    \n\n\n\n\n\nBulk Updates\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u274c Not idempotent\n\n\n\n\nTo change parts of a resource or resources use the \nPATCH\n verb.\nFor instance, here is how to mark all young people as children.\n\n\nPATCH /people?age=lt.13\n{\n  \nperson_type\n: \nchild\n\n}\n\n\n\n\nThis affects any rows matched by the url param filters, overwrites\nany fields specified in in the payload JSON and leaves the other\nfields unaffected. Note that although the payload is not in the\nJSON patch format specified by\n\nRFC6902\n, HTTP does not specify\nwhich patch format to use. Our format is more pleasant, meant for\nbasic field replacements, and not at all \"incorrect.\"\n\n\nDeletion\n\n\n\n\n\u274c Cannot be cached or prefetched\n\n\n\u2705 Idempotent\n\n\n\n\nSimply use the \nDELETE\n verb. All records that match your filter\nwill be removed. For instance deleting inactive users:\n\n\nDELETE /user?active=is.false\n\n\n\n\nProtecting Dangerous Actions\n\n\nNotice that it is very easy to delete or update many records at\nonce. In fact forgetting a filter will affect an entire table!\n\n\n\n    \nInvitation to Contribute\n\n\n    \nWe would like to investigate nginx rules to guard dangerous\n    actions, perhaps requiring a confirmation header or query param\n    to perform the action.\n\n\n    \nYou're invited to research this option and contribute to\n    this documentation.", 
            "title": "Writing"
        }, 
        {
            "location": "/api/writing/#updating-data", 
            "text": "", 
            "title": "Updating Data"
        }, 
        {
            "location": "/api/writing/#record-creation", 
            "text": "\u274c Cannot be cached or prefetched  \u274c Not idempotent   To create a row in a database table post a JSON object whose keys\nare the names of the columns you would like to create. Missing keys\nwill be set to default values when applicable.  POST /table_name\n{  col1 :  value1 ,  col2 :  value2  }  The response will include a  Location  header describing where to\nfind the new object. If you would like to get the full object back\nin the response to your request, include the header  Prefer:\nreturn=representation . That way you won't have to make another\nHTTP call to discover properties that may have been filled in on\nthe server side.", 
            "title": "Record Creation"
        }, 
        {
            "location": "/api/writing/#bulk-insertion", 
            "text": "\u274c Cannot be cached or prefetched  \u274c Not idempotent   You can POST a JSON array or CSV to insert multiple rows in a single\nHTTP request. Note that using CSV requires less parsing on the server\nand is  much faster .  Example of CSV bulk insert. Simply post to a table route with Content-Type: text/csv  and include the names of the columns as\nthe first row. For instance  POST /people\nname,age,height\nJ Doe,62,70\nJonas,10,55  An empty field ( ,, ) is coerced to an empty string and the reserved\nword  NULL  is mapped to the SQL null value. Note that there should\nbe no spaces between the column names and commas.  Example of JSON bulk insert. Send an array:  POST /people\n[\n  {  name :  J Doe ,  age : 62,  height : 70 },\n  {  name :  Janus ,  age : 10,  height : 55 }\n]  If you would like to get the full object back in the response to\nyour request, include the header  Prefer: return=representation .\nChances are you only want certain information back, though, like\ncreated ids. You can pass a  select  parameter to affect the shape\nof the response (further documented in the  reading \npage). For instance  POST /people?select=id\n[...]  returns something like  [ {  id : 1 }, {  id : 2 } ]", 
            "title": "Bulk Insertion"
        }, 
        {
            "location": "/api/writing/#multiple-tables-insertion-or-update", 
            "text": "The cleanest way to insert or update data into multiple tables using only one POST/PATCH request\nis to create a view that will join all target tables and present a single endpoint.\nIn our example let's assume one users table and one companies table.\nIn this case, we want a signup endpoint to create the first user within a company.\nAnd for this endpoint we want to insert with one request both user and company.  CREATE TABLE companies (\n    id serial primary key,\n    name text unique\n);\n\nCREATE TABLE users (\n    id serial primary key,\n    name text not null,\n    pass text,\n    company_id integer not null references companies\n);  Having both tables created we create a view that joins them to be used\nas a  /signup  endpoint.  CREATE VIEW signup AS\n    SELECT\n        c.name AS company_name,\n        u.name AS user_name,\n        u.pass\n    FROM\n        public.users u\n        JOIN public.companies c ON c.id = u.company_id;  After the signup view creation, we can issue  GET  requests to read data\nfrom users and companies, but any atempt to  POST  or  PATCH  data will fail.\nPostgreSQL won't allow any data change on views that have a  JOIN  \nclause in their  FROM  without a proper  INSTEAD OF  trigger.\nSo in the example bellow we create a trigger to allow insertion of data in the signup view.\nThe trigger is a simple PL/pgSQL function that first inserts into the companies table and\nuses the newly create company_id to create its first user.  CREATE FUNCTION signup()\nRETURNS trigger\nLANGUAGE plpgsql\nAS $$\nDECLARE\n  vcompany_id int;\nBEGIN\n  INSERT INTO companies (name) VALUES (new.company_name) RETURNING id INTO vcompany_id;\n  INSERT INTO users (name, pass, company_id) VALUES (new.user_name, new.pass, vcompany_id);\nRETURN new;\nEND;\n$$;\n\nCREATE TRIGGER signup\nINSTEAD OF INSERT ON signup\nFOR EACH ROW\nEXECUTE PROCEDURE signup();  After the trigger creation we can issue a normal  POST  request to our signup endpoint:  POST /signup\n{  company_name :  foo ,  user_name :  bar  }  For an endpoint such as signup its usually not desirable to have a  PATCH  route for updates,\nand we will skip this example for the sake of brevity. But it would be implemented in a very\nsimilar way to our  POST  example.  \n     Design Consideration \n\n     It's advisable to create a separate trigger for  UPDATE  and  INSERT \n    avoiding conditionals that decide which is the trigger current operation.\n    This makes it easier to change code for (or even disable) one operation without interfering with others while\n    improving readability.", 
            "title": "Multiple Tables Insertion or Update"
        }, 
        {
            "location": "/api/writing/#bulk-updates", 
            "text": "\u274c Cannot be cached or prefetched  \u274c Not idempotent   To change parts of a resource or resources use the  PATCH  verb.\nFor instance, here is how to mark all young people as children.  PATCH /people?age=lt.13\n{\n   person_type :  child \n}  This affects any rows matched by the url param filters, overwrites\nany fields specified in in the payload JSON and leaves the other\nfields unaffected. Note that although the payload is not in the\nJSON patch format specified by RFC6902 , HTTP does not specify\nwhich patch format to use. Our format is more pleasant, meant for\nbasic field replacements, and not at all \"incorrect.\"", 
            "title": "Bulk Updates"
        }, 
        {
            "location": "/api/writing/#deletion", 
            "text": "\u274c Cannot be cached or prefetched  \u2705 Idempotent   Simply use the  DELETE  verb. All records that match your filter\nwill be removed. For instance deleting inactive users:  DELETE /user?active=is.false", 
            "title": "Deletion"
        }, 
        {
            "location": "/api/writing/#protecting-dangerous-actions", 
            "text": "Notice that it is very easy to delete or update many records at\nonce. In fact forgetting a filter will affect an entire table!  \n     Invitation to Contribute \n\n     We would like to investigate nginx rules to guard dangerous\n    actions, perhaps requiring a confirmation header or query param\n    to perform the action. \n\n     You're invited to research this option and contribute to\n    this documentation.", 
            "title": "Protecting Dangerous Actions"
        }, 
        {
            "location": "/admin/security/", 
            "text": "Security\n\n\nPostgREST is designed to keep the database at the center of API\nsecurity. All authorization happens through database roles and\npermissions. It is PostgREST's job to \nauthenticate\n requests --\ni.e. verify that a client is who they say they are -- and then let\nthe database \nauthorize\n client actions.\n\n\nWe use \nJSON Web Tokens\n to authenticate API requests.\nAs you'll recall a JWT contains a list of cryptographically signed\nclaims. PostgREST cares specifically about a claim called \nrole\n.\nWhen request contains a valid JWT with a role claim PostgREST will\nswitch to the database role with that name for the duration of the\nHTTP request.  If the client included no (or an invalid) JWT then\nPostgREST selects the \"anonymous role\" which is specified by a\ncommand line arguments to the server on startup.\n\n\n{\n  \nrole\n: \njdoe123\n\n}\n\n// Encoded as JWT with a secret of \nsecret\n this becomes\n// eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiamRvZTEyMyJ9.X_ZeWSS9qsKDCDczv8C-GE2fccrPQjOh_ALMZJa5jsU\n\n\n\n\nUsing JWT allows us to authenticate with external services. A login\nservice needs merely to share a JWT encryption secret with the\nPostgREST server. The secret is also a server command line option.\n\n\nIt is even possible to generate JWT from inside a stored procedure\nin your database. Any SQL stored procedure that returns a type whose\nname ends in \njwt_claims\n will have its return value encoded into\nJWT.  See the \nUser Management\n\nexample for details.\n\n\nDatabase Roles\n\n\nSuppose you start the server like this:\n\n\npostgrest postgres://foo@localhost:5432/mydb --anonymous anon\n\n\n\n\nThis means that \nfoo\n is the so-called \nauthenticator role\n and\n\nanon\n is the anonymous role. When a new HTTP request arrives at the\nserver the latter is connected to the database as user \nfoo\n. If\nno JWT is present, or if it is invalid, or if it does not contain\nthe role claim then the server changes to the anonymous role with\nthe query\n\n\nSET LOCAL ROLE anon;\n\n\n\n\nOtherwise it sets the role to that specified by JWT. For security\nyour authenticator role should have access to nothing except the\nability to become other users. Supposing you have three roles, one\nfor anonymous users, one for authors, and another for the authenticator,\nyou would set it up like this\n\n\nCREATE ROLE authenticator NOINHERIT LOGIN;\nCREATE ROLE anon;\nCREATE ROLE author;\n\nGRANT anon, author TO authenticator;\n\n\n\n\nRow-Level Security\n\n\nSimulated - PostgreSQL \n9.5\n\n\nReal - PostgreSQL \n=9.5\n\n\nBuilding Auth on top of JWT\n\n\nBasic Auth\n\n\nGithub Sign-in\n\n\nSSL", 
            "title": "Security"
        }, 
        {
            "location": "/admin/security/#security", 
            "text": "PostgREST is designed to keep the database at the center of API\nsecurity. All authorization happens through database roles and\npermissions. It is PostgREST's job to  authenticate  requests --\ni.e. verify that a client is who they say they are -- and then let\nthe database  authorize  client actions.  We use  JSON Web Tokens  to authenticate API requests.\nAs you'll recall a JWT contains a list of cryptographically signed\nclaims. PostgREST cares specifically about a claim called  role .\nWhen request contains a valid JWT with a role claim PostgREST will\nswitch to the database role with that name for the duration of the\nHTTP request.  If the client included no (or an invalid) JWT then\nPostgREST selects the \"anonymous role\" which is specified by a\ncommand line arguments to the server on startup.  {\n   role :  jdoe123 \n}\n\n// Encoded as JWT with a secret of  secret  this becomes\n// eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiamRvZTEyMyJ9.X_ZeWSS9qsKDCDczv8C-GE2fccrPQjOh_ALMZJa5jsU  Using JWT allows us to authenticate with external services. A login\nservice needs merely to share a JWT encryption secret with the\nPostgREST server. The secret is also a server command line option.  It is even possible to generate JWT from inside a stored procedure\nin your database. Any SQL stored procedure that returns a type whose\nname ends in  jwt_claims  will have its return value encoded into\nJWT.  See the  User Management \nexample for details.", 
            "title": "Security"
        }, 
        {
            "location": "/admin/security/#database-roles", 
            "text": "Suppose you start the server like this:  postgrest postgres://foo@localhost:5432/mydb --anonymous anon  This means that  foo  is the so-called  authenticator role  and anon  is the anonymous role. When a new HTTP request arrives at the\nserver the latter is connected to the database as user  foo . If\nno JWT is present, or if it is invalid, or if it does not contain\nthe role claim then the server changes to the anonymous role with\nthe query  SET LOCAL ROLE anon;  Otherwise it sets the role to that specified by JWT. For security\nyour authenticator role should have access to nothing except the\nability to become other users. Supposing you have three roles, one\nfor anonymous users, one for authors, and another for the authenticator,\nyou would set it up like this  CREATE ROLE authenticator NOINHERIT LOGIN;\nCREATE ROLE anon;\nCREATE ROLE author;\n\nGRANT anon, author TO authenticator;", 
            "title": "Database Roles"
        }, 
        {
            "location": "/admin/security/#row-level-security", 
            "text": "", 
            "title": "Row-Level Security"
        }, 
        {
            "location": "/admin/security/#simulated-postgresql-95", 
            "text": "", 
            "title": "Simulated - PostgreSQL &lt;9.5"
        }, 
        {
            "location": "/admin/security/#real-postgresql-95", 
            "text": "", 
            "title": "Real - PostgreSQL &gt;=9.5"
        }, 
        {
            "location": "/admin/security/#building-auth-on-top-of-jwt", 
            "text": "", 
            "title": "Building Auth on top of JWT"
        }, 
        {
            "location": "/admin/security/#basic-auth", 
            "text": "", 
            "title": "Basic Auth"
        }, 
        {
            "location": "/admin/security/#github-sign-in", 
            "text": "", 
            "title": "Github Sign-in"
        }, 
        {
            "location": "/admin/security/#ssl", 
            "text": "", 
            "title": "SSL"
        }, 
        {
            "location": "/admin/versioning/", 
            "text": "API Versioning\n\n\nSchema Search Path\n\n\nChanging a Resource\n\n\nRemoving a Resource\n\n\nAvoiding DB and Client Coupling", 
            "title": "Versioning"
        }, 
        {
            "location": "/admin/versioning/#api-versioning", 
            "text": "", 
            "title": "API Versioning"
        }, 
        {
            "location": "/admin/versioning/#schema-search-path", 
            "text": "", 
            "title": "Schema Search Path"
        }, 
        {
            "location": "/admin/versioning/#changing-a-resource", 
            "text": "", 
            "title": "Changing a Resource"
        }, 
        {
            "location": "/admin/versioning/#removing-a-resource", 
            "text": "", 
            "title": "Removing a Resource"
        }, 
        {
            "location": "/admin/versioning/#avoiding-db-and-client-coupling", 
            "text": "", 
            "title": "Avoiding DB and Client Coupling"
        }, 
        {
            "location": "/admin/migration/", 
            "text": "Data Migration\n\n\nSqitch\n\n\nTest-Driven Migrations\n\n\nStructural Tests\n\n\nValue Tests with pgTAP", 
            "title": "Migration"
        }, 
        {
            "location": "/admin/migration/#data-migration", 
            "text": "", 
            "title": "Data Migration"
        }, 
        {
            "location": "/admin/migration/#sqitch", 
            "text": "", 
            "title": "Sqitch"
        }, 
        {
            "location": "/admin/migration/#test-driven-migrations", 
            "text": "", 
            "title": "Test-Driven Migrations"
        }, 
        {
            "location": "/admin/migration/#structural-tests", 
            "text": "", 
            "title": "Structural Tests"
        }, 
        {
            "location": "/admin/migration/#value-tests-with-pgtap", 
            "text": "", 
            "title": "Value Tests with pgTAP"
        }, 
        {
            "location": "/admin/deployment/", 
            "text": "Deployment\n\n\nHeroku\n\n\nGetting Started\n\n\nUsing Amazon RDS\n\n\nDebian", 
            "title": "Deployment"
        }, 
        {
            "location": "/admin/deployment/#deployment", 
            "text": "", 
            "title": "Deployment"
        }, 
        {
            "location": "/admin/deployment/#heroku", 
            "text": "", 
            "title": "Heroku"
        }, 
        {
            "location": "/admin/deployment/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/admin/deployment/#using-amazon-rds", 
            "text": "", 
            "title": "Using Amazon RDS"
        }, 
        {
            "location": "/admin/deployment/#debian", 
            "text": "", 
            "title": "Debian"
        }, 
        {
            "location": "/admin/performance/", 
            "text": "Performance\n\n\nBenchmarks\n\n\nCaching\n\n\nQuality of Service\n\n\nTips", 
            "title": "Performance"
        }, 
        {
            "location": "/admin/performance/#performance", 
            "text": "", 
            "title": "Performance"
        }, 
        {
            "location": "/admin/performance/#benchmarks", 
            "text": "", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/admin/performance/#caching", 
            "text": "", 
            "title": "Caching"
        }, 
        {
            "location": "/admin/performance/#quality-of-service", 
            "text": "", 
            "title": "Quality of Service"
        }, 
        {
            "location": "/admin/performance/#tips", 
            "text": "", 
            "title": "Tips"
        }, 
        {
            "location": "/examples/start/", 
            "text": "Getting Started\n\n\nYour First (simple) API\n\n\nLet's start with the simplest thing possible. We will expose some tables directly for reading and writing by anyone.\n\n\nStart by making a database\n\n\ncreatedb demo1\n\n\n\n\nWe'll set it up with a film example (courtesy of \nJonathan Harrington\n). Copy the following into your clipboard:\n\n\nBEGIN;\n\nCREATE TABLE director\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE film\n(\n  id serial PRIMARY KEY,\n  title text NOT NULL,\n  year date NOT NULL,\n  director text REFERENCES director (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  rating real NOT NULL DEFAULT 0,\n  language text NOT NULL\n);\n\nCREATE TABLE festival\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE competition\n(\n  id serial PRIMARY KEY,\n  name text NOT NULL,\n  festival text NOT NULL REFERENCES festival (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  year date NOT NULL\n);\n\nCREATE TABLE film_nomination\n(\n  id serial PRIMARY KEY,\n  competition integer NOT NULL REFERENCES competition (id)\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  film integer NOT NULL REFERENCES film (id)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  won boolean NOT NULL DEFAULT true\n);\n\nCOMMIT;\n\n\n\n\nApply it to your new database by running\n\n\n# On OS X\npbpaste | psql demo1\n\n# Or Linux\n# xclip -selection clipboard -o | psql demo1\n\n\n\n\nStart the PostgREST server and point it at the new database. (See the \ninstallation instructions\n.)\n\n\npostgrest postgres://postgres:@localhost:5432/demo1 -a postgres --schema public\n\n\n\n\n\n    \nNote about database users\n\n\n    \nIf you installed PostgreSQL with Homebrew on Mac then the\n    database username may be your own login rather than\n    \npostgres\n.\n\n\n\n\n\nPopulating Data\n\n\nLet's use PostgREST to populate the database. Install a REST client such as \nPostman\n. Now let's insert some data as a bulk post in CSV format:\n\n\nPOST http://localhost:3000/festival\nContent-Type: text/csv\n\nname\nVenice Film Festival\nCannes Film Festival\n\n\n\n\nIn Postman it will look like this\n\n\n\n\nNotice that the post type is \nraw\n and that \nContent-Type: text/csv\n set in the Headers tab.\n\n\nThe server returns HTTP 201 Created. Because we inserted more than one item at once there is no \nLocation\n header in the response. However sometimes you want to learn more about items which you just inserted. To have the server include the full results, include the header \nPrefer: return=representation\n.\n\n\nAt this point if you send a GET request to \n/festival\n it should return\n\n\n[\n  {\n    \nname\n: \nVenice Film Festival\n\n  },\n  {\n    \nname\n: \nCannes Film Festival\n\n  }\n]\n\n\n\n\nNow that you've seen how to do a bulk insert, let's do some more and fully populate the database.\n\n\nPost the following to \n/competition\n:\n\n\nname,festival,year\nGolden Lion,Venice Film Festival,2014-01-01\nPalme d'Or,Cannes Film Festival,2014-01-01\n\n\n\n\nNow \n/director\n:\n\n\nname\nBertrand Bonello\nAtom Egoyan\nDavid Gordon Green\nAndrey Konchalovskiy\nMario Martone\nMike Leigh\nRoy Andersson\nSaverio Costanzo\nAlix Delaporte\nJean-Pierre Dardenne\nXiaoshuai Wang\nKaan M\u00fcjdeci\nTommy Lee Jones\nNuri Bilge Ceylan\nMichel Hazanavicius\nXavier Dolan\nRamin Bahrani\nAlice Rohrwacher\nAndrew Niccol\nRakhshan Bani-Etemad\nDavid Oelhoffen\nBennett Miller\nDavid Cronenberg\nShin'ya Tsukamoto\nJoshua Oppenheimer\nOlivier Assayas\nJean-Luc Godard\nAlejandro Gonz\u00e1lez I\u00f1\u00e1rritu\nBeno\u00eet Jacquot\nFatih Akin\nFrancesco Munzi\nKen Loach\nAbel Ferrara\nXavier Beauvois\nNaomi Kawase\n\n\n\n\nAnd \n/film\n:\n\n\ntitle,year,director,rating,language\nChuang ru zhe,2014-01-01,Xiaoshuai Wang,6.19999981,english\nThe Look of Silence,2014-01-01,Joshua Oppenheimer,8.30000019,Indonesian\nFires on the Plain,2014-01-01,Shin'ya Tsukamoto,5.80000019,Japanese\nFar from Men,2014-01-01,David Oelhoffen,7.5,english\nGood Kill,2014-01-01,Andrew Niccol,6.0999999,english\nLeopardi,2014-01-01,Mario Martone,6.9000001,english\nSivas,2014-01-01,Kaan M\u00fcjdeci,7.69999981,english\nBlack Souls,2014-01-01,Francesco Munzi,7.0999999,english\nThree Hearts,2014-01-01,Beno\u00eet Jacquot,5.80000019,French\nPasolini,2014-01-01,Abel Ferrara,5.80000019,english\nLe dernier coup de marteau,2014-01-01,Alix Delaporte,6.5,english\nManglehorn,2014-01-01,David Gordon Green,7.0999999,english\nHungry Hearts,2014-01-01,Saverio Costanzo,6.4000001,English\nBelye nochi pochtalona Alekseya Tryapitsyna,2014-01-01,Andrey Konchalovskiy,6.9000001,Russian\n99 Homes,2014-01-01,Ramin Bahrani,7.30000019,english\nThe Cut,2014-01-01,Fatih Akin,6,Armenian\nBirdman: Or (The Unexpected Virtue of Ignorance),2014-01-01,Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu,8,English\nLa ran\u00e7on de la gloire,2014-01-01,Xavier Beauvois,5.69999981,French\nA Pigeon Sat on a Branch Reflecting on Existence,2014-01-01,Roy Andersson,7.19999981,english\nTales,2014-01-01,Rakhshan Bani-Etemad,6.80000019,english\nThe Wonders,2014-01-01,Alice Rohrwacher,6.80000019,Italian\nFoxcatcher,2014-01-01,Bennett Miller,7.19999981,English\nMr. Turner,2014-01-01,Mike Leigh,7,English\nJimmy's Hall,2014-01-01,Ken Loach,6.69999981,English\nThe Homesman,2014-01-01,Tommy Lee Jones,6.5999999,English\nThe Captive,2014-01-01,Atom Egoyan,5.9000001,english\nGoodbye to Language,2014-01-01,Jean-Luc Godard,6.19999981,French\nThe Search,2014-01-01,Michel Hazanavicius,6.9000001,French\nStill the Water,2014-01-01,Naomi Kawase,6.9000001,Japanese\nMommy,2014-01-01,Xavier Dolan,8.30000019,French\n\nTwo Days, One Night\n,2014-01-01,Jean-Pierre Dardenne,7.4000001,French\nMaps to the Stars,2014-01-01,David Cronenberg,6.4000001,English\nSaint Laurent,2014-01-01,Bertrand Bonello,6.5,French\nClouds of Sils Maria,2014-01-01,Olivier Assayas,6.9000001,english\nWinter Sleep,2014-01-01,Nuri Bilge Ceylan,8.5,Turkish\n\n\n\n\nFinally \n/film_nomination\n:\n\n\ncompetition,film,won\n1,1,f\n1,2,f\n1,3,f\n1,4,f\n1,5,f\n1,6,f\n1,7,f\n1,8,f\n1,9,f\n1,10,f\n1,11,f\n1,12,f\n1,13,f\n1,14,f\n1,15,f\n1,16,f\n1,17,f\n1,18,f\n1,19,f\n1,20,f\n2,21,f\n2,22,f\n2,23,f\n2,24,f\n2,25,f\n2,26,f\n2,27,f\n2,28,f\n2,29,f\n2,30,f\n2,31,f\n2,32,f\n2,33,f\n2,34,f\n2,35,f\n\n\n\n\nGetting and Embedding Data\n\n\nFirst let's review which films are stored in the database:\n\n\nGET http://localhost:3000/film\n\n\n\n\nIt gives us back a list of JSON objects. What if we care only about the film titles? Use \nselect\n to shape the output:\n\n\nGET http://localhost:3000/film?select=title\n\n\n\n\n[\n  {\n    \ntitle\n: \nChuang ru zhe\n\n  },\n  {\n    \ntitle\n: \nThe Look of Silence\n\n  },\n  {\n    \ntitle\n: \nFires on the Plain\n\n  },\n  ...\n]\n\n\n\n\nHere is where it gets cool. PostgREST can embed objects in its response through foreign key relationships. Earlier we created a join table called \nfilm_nomination\n. It joins films and competitions. We can ask the server about the structure of this table:\n\n\nOPTIONS http://localhost:3000/film_nomination\n\n\n\n\n{\n  \npkey\n: [\n    \nid\n\n  ],\n  \ncolumns\n: [\n    {\n      \nreferences\n: null,\n      \ndefault\n: \nnextval('film_nomination_id_seq'::regclass)\n,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nid\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 1\n    },\n    {\n      \nreferences\n: {\n        \nschema\n: \npublic\n,\n        \ncolumn\n: \nid\n,\n        \ntable\n: \ncompetition\n\n      },\n      \ndefault\n: null,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \ncompetition\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 2\n    },\n    {\n      \nreferences\n: {\n        \nschema\n: \npublic\n,\n        \ncolumn\n: \nid\n,\n        \ntable\n: \nfilm\n\n      },\n      \ndefault\n: null,\n      \nprecision\n: 32,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nfilm\n,\n      \ntype\n: \ninteger\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 3\n    },\n    {\n      \nreferences\n: null,\n      \ndefault\n: \ntrue\n,\n      \nprecision\n: null,\n      \nupdatable\n: true,\n      \nschema\n: \npublic\n,\n      \nname\n: \nwon\n,\n      \ntype\n: \nboolean\n,\n      \nmaxLen\n: null,\n      \nenum\n: [],\n      \nnullable\n: false,\n      \nposition\n: 4\n    }\n  ]\n}\n\n\n\n\nFrom this you can see that the columns \nfilm\n and \ncompetition\n reference their eponymous tables. Let's ask the server for each film along with names of the competitions it entered. You don't have to do any custom coding. Send this query:\n\n\nGET http://localhost:3000/film?select=title,competition{name}\n\n\n\n\n[\n  {\n    \ntitle\n: \nChuang ru zhe\n,\n    \ncompetition\n: [\n      {\n        \nname\n: \nGolden Lion\n\n      }\n    ]\n  },\n  {\n    \ntitle\n: \nThe Look of Silence\n,\n    \ncompetition\n: [\n      {\n        \nname\n: \nGolden Lion\n\n      }\n    ]\n  },\n  ...\n]\n\n\n\n\nThe relation flows both ways. Here is how to get the name of each competition's name and the movies shown at it.\n\n\nGET http://localhost:3000/competition?select=name,film{title}\n\n\n\n\n[\n  {\n    \nname\n: \nGolden Lion\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nChuang ru zhe\n\n      },\n      {\n        \ntitle\n: \nThe Look of Silence\n\n      },\n      ...\n    ]\n  },\n  {\n    \nname\n: \nPalme d'Or\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nThe Wonders\n\n      },\n      {\n        \ntitle\n: \nFoxcatcher\n\n      },\n      ...\n    ]\n  }\n]\n\n\n\n\nWhy not learn about the directors too? There is a many-to-one relation directly between films and directors. We can alter our previous query to include directors in its results.\n\n\nGET http://localhost:3000/competition?select=name,film{title,director{*}}\n\n\n\n\n[\n  {\n    \nname\n: \nGolden Lion\n,\n    \nfilm\n: [\n      {\n        \ntitle\n: \nManglehorn\n,\n        \ndirector\n: {\n          \nname\n: \nDavid Gordon Green\n\n        }\n      },\n      {\n        \ntitle\n: \nBelye nochi pochtalona Alekseya Tryapitsyna\n,\n        \ndirector\n: {\n          \nname\n: \nAndrey Konchalovskiy\n\n        }\n      },\n      ...\n    ]\n  },\n  ...\n]\n\n\n\n\nSingular Responses\n\n\nHow do we ask for a single film, for instance the second one we inserted?\n\n\nGET http://localhost:3000/film?id=eq.2\n\n\n\n\nIt returns\n\n\n[\n  {\n    \nid\n: 2,\n    \ntitle\n: \nThe Look of Silence\n,\n    \nyear\n: \n2014-01-01\n,\n    \ndirector\n: \nJoshua Oppenheimer\n,\n    \nrating\n: 8.3,\n    \nlanguage\n: \nIndonesian\n\n  }\n]\n\n\n\n\nLike any query, it gives us a result \nset\n, in this case an array with one element. However you and I know that \nid\n is a primary key, it will never return more than one result. We might want it returned as a JSON object, not an array. To express this preference include the header \nPrefer: plurality=singular\n. It will respond with\n\n\n{\n  \nid\n: 2,\n  \ntitle\n: \nThe Look of Silence\n,\n  \nyear\n: \n2014-01-01\n,\n  \ndirector\n: \nJoshua Oppenheimer\n,\n  \nrating\n: 8.3,\n  \nlanguage\n: \nIndonesian\n\n}\n\n\n\n\n\n    \nWhy this approach to singular responses?\n\n\n    \n\n    PostgREST knows which columns comprise a primary key for a\n    table, so why not automatically choose plurality=singular when\n    these column filters are present? The fact is it could come as a\n    shock to a client that by adding one more filter condition it can\n    change the entire response format.\n    \n\n    \n\n    Then why not expose another kind of route such as /film/2 to indicate\n    one particular film? Because this does not accommodate compound keys.\n    The convention complects a plurality preference with table key\n    assumptions. We should separate concerns.\n    \n\n    \n\n    It turns out you can still have routes like /film/2.  Use a\n    proxy such as Nginx. It can rewrite routes such as /films/2\n    into /films?id=eq.2 and add the Prefer header to make the results\n    singular.\n    \n\n\n\n\n\nConclusion\n\n\nThis tutorial showed how to create a database with a basic schema, run PostgREST, and interact with the API. The next tutorial will show how to enable security for a multi-tenant blogging API.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/examples/start/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/examples/start/#your-first-simple-api", 
            "text": "Let's start with the simplest thing possible. We will expose some tables directly for reading and writing by anyone.  Start by making a database  createdb demo1  We'll set it up with a film example (courtesy of  Jonathan Harrington ). Copy the following into your clipboard:  BEGIN;\n\nCREATE TABLE director\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE film\n(\n  id serial PRIMARY KEY,\n  title text NOT NULL,\n  year date NOT NULL,\n  director text REFERENCES director (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  rating real NOT NULL DEFAULT 0,\n  language text NOT NULL\n);\n\nCREATE TABLE festival\n(\n  name text NOT NULL PRIMARY KEY\n);\n\nCREATE TABLE competition\n(\n  id serial PRIMARY KEY,\n  name text NOT NULL,\n  festival text NOT NULL REFERENCES festival (name)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  year date NOT NULL\n);\n\nCREATE TABLE film_nomination\n(\n  id serial PRIMARY KEY,\n  competition integer NOT NULL REFERENCES competition (id)\n    ON UPDATE NO ACTION ON DELETE NO ACTION,\n  film integer NOT NULL REFERENCES film (id)\n    ON UPDATE CASCADE ON DELETE CASCADE,\n  won boolean NOT NULL DEFAULT true\n);\n\nCOMMIT;  Apply it to your new database by running  # On OS X\npbpaste | psql demo1\n\n# Or Linux\n# xclip -selection clipboard -o | psql demo1  Start the PostgREST server and point it at the new database. (See the  installation instructions .)  postgrest postgres://postgres:@localhost:5432/demo1 -a postgres --schema public  \n     Note about database users \n\n     If you installed PostgreSQL with Homebrew on Mac then the\n    database username may be your own login rather than\n     postgres .", 
            "title": "Your First (simple) API"
        }, 
        {
            "location": "/examples/start/#populating-data", 
            "text": "Let's use PostgREST to populate the database. Install a REST client such as  Postman . Now let's insert some data as a bulk post in CSV format:  POST http://localhost:3000/festival\nContent-Type: text/csv\n\nname\nVenice Film Festival\nCannes Film Festival  In Postman it will look like this   Notice that the post type is  raw  and that  Content-Type: text/csv  set in the Headers tab.  The server returns HTTP 201 Created. Because we inserted more than one item at once there is no  Location  header in the response. However sometimes you want to learn more about items which you just inserted. To have the server include the full results, include the header  Prefer: return=representation .  At this point if you send a GET request to  /festival  it should return  [\n  {\n     name :  Venice Film Festival \n  },\n  {\n     name :  Cannes Film Festival \n  }\n]  Now that you've seen how to do a bulk insert, let's do some more and fully populate the database.  Post the following to  /competition :  name,festival,year\nGolden Lion,Venice Film Festival,2014-01-01\nPalme d'Or,Cannes Film Festival,2014-01-01  Now  /director :  name\nBertrand Bonello\nAtom Egoyan\nDavid Gordon Green\nAndrey Konchalovskiy\nMario Martone\nMike Leigh\nRoy Andersson\nSaverio Costanzo\nAlix Delaporte\nJean-Pierre Dardenne\nXiaoshuai Wang\nKaan M\u00fcjdeci\nTommy Lee Jones\nNuri Bilge Ceylan\nMichel Hazanavicius\nXavier Dolan\nRamin Bahrani\nAlice Rohrwacher\nAndrew Niccol\nRakhshan Bani-Etemad\nDavid Oelhoffen\nBennett Miller\nDavid Cronenberg\nShin'ya Tsukamoto\nJoshua Oppenheimer\nOlivier Assayas\nJean-Luc Godard\nAlejandro Gonz\u00e1lez I\u00f1\u00e1rritu\nBeno\u00eet Jacquot\nFatih Akin\nFrancesco Munzi\nKen Loach\nAbel Ferrara\nXavier Beauvois\nNaomi Kawase  And  /film :  title,year,director,rating,language\nChuang ru zhe,2014-01-01,Xiaoshuai Wang,6.19999981,english\nThe Look of Silence,2014-01-01,Joshua Oppenheimer,8.30000019,Indonesian\nFires on the Plain,2014-01-01,Shin'ya Tsukamoto,5.80000019,Japanese\nFar from Men,2014-01-01,David Oelhoffen,7.5,english\nGood Kill,2014-01-01,Andrew Niccol,6.0999999,english\nLeopardi,2014-01-01,Mario Martone,6.9000001,english\nSivas,2014-01-01,Kaan M\u00fcjdeci,7.69999981,english\nBlack Souls,2014-01-01,Francesco Munzi,7.0999999,english\nThree Hearts,2014-01-01,Beno\u00eet Jacquot,5.80000019,French\nPasolini,2014-01-01,Abel Ferrara,5.80000019,english\nLe dernier coup de marteau,2014-01-01,Alix Delaporte,6.5,english\nManglehorn,2014-01-01,David Gordon Green,7.0999999,english\nHungry Hearts,2014-01-01,Saverio Costanzo,6.4000001,English\nBelye nochi pochtalona Alekseya Tryapitsyna,2014-01-01,Andrey Konchalovskiy,6.9000001,Russian\n99 Homes,2014-01-01,Ramin Bahrani,7.30000019,english\nThe Cut,2014-01-01,Fatih Akin,6,Armenian\nBirdman: Or (The Unexpected Virtue of Ignorance),2014-01-01,Alejandro Gonz\u00e1lez I\u00f1\u00e1rritu,8,English\nLa ran\u00e7on de la gloire,2014-01-01,Xavier Beauvois,5.69999981,French\nA Pigeon Sat on a Branch Reflecting on Existence,2014-01-01,Roy Andersson,7.19999981,english\nTales,2014-01-01,Rakhshan Bani-Etemad,6.80000019,english\nThe Wonders,2014-01-01,Alice Rohrwacher,6.80000019,Italian\nFoxcatcher,2014-01-01,Bennett Miller,7.19999981,English\nMr. Turner,2014-01-01,Mike Leigh,7,English\nJimmy's Hall,2014-01-01,Ken Loach,6.69999981,English\nThe Homesman,2014-01-01,Tommy Lee Jones,6.5999999,English\nThe Captive,2014-01-01,Atom Egoyan,5.9000001,english\nGoodbye to Language,2014-01-01,Jean-Luc Godard,6.19999981,French\nThe Search,2014-01-01,Michel Hazanavicius,6.9000001,French\nStill the Water,2014-01-01,Naomi Kawase,6.9000001,Japanese\nMommy,2014-01-01,Xavier Dolan,8.30000019,French Two Days, One Night ,2014-01-01,Jean-Pierre Dardenne,7.4000001,French\nMaps to the Stars,2014-01-01,David Cronenberg,6.4000001,English\nSaint Laurent,2014-01-01,Bertrand Bonello,6.5,French\nClouds of Sils Maria,2014-01-01,Olivier Assayas,6.9000001,english\nWinter Sleep,2014-01-01,Nuri Bilge Ceylan,8.5,Turkish  Finally  /film_nomination :  competition,film,won\n1,1,f\n1,2,f\n1,3,f\n1,4,f\n1,5,f\n1,6,f\n1,7,f\n1,8,f\n1,9,f\n1,10,f\n1,11,f\n1,12,f\n1,13,f\n1,14,f\n1,15,f\n1,16,f\n1,17,f\n1,18,f\n1,19,f\n1,20,f\n2,21,f\n2,22,f\n2,23,f\n2,24,f\n2,25,f\n2,26,f\n2,27,f\n2,28,f\n2,29,f\n2,30,f\n2,31,f\n2,32,f\n2,33,f\n2,34,f\n2,35,f", 
            "title": "Populating Data"
        }, 
        {
            "location": "/examples/start/#getting-and-embedding-data", 
            "text": "First let's review which films are stored in the database:  GET http://localhost:3000/film  It gives us back a list of JSON objects. What if we care only about the film titles? Use  select  to shape the output:  GET http://localhost:3000/film?select=title  [\n  {\n     title :  Chuang ru zhe \n  },\n  {\n     title :  The Look of Silence \n  },\n  {\n     title :  Fires on the Plain \n  },\n  ...\n]  Here is where it gets cool. PostgREST can embed objects in its response through foreign key relationships. Earlier we created a join table called  film_nomination . It joins films and competitions. We can ask the server about the structure of this table:  OPTIONS http://localhost:3000/film_nomination  {\n   pkey : [\n     id \n  ],\n   columns : [\n    {\n       references : null,\n       default :  nextval('film_nomination_id_seq'::regclass) ,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  id ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 1\n    },\n    {\n       references : {\n         schema :  public ,\n         column :  id ,\n         table :  competition \n      },\n       default : null,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  competition ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 2\n    },\n    {\n       references : {\n         schema :  public ,\n         column :  id ,\n         table :  film \n      },\n       default : null,\n       precision : 32,\n       updatable : true,\n       schema :  public ,\n       name :  film ,\n       type :  integer ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 3\n    },\n    {\n       references : null,\n       default :  true ,\n       precision : null,\n       updatable : true,\n       schema :  public ,\n       name :  won ,\n       type :  boolean ,\n       maxLen : null,\n       enum : [],\n       nullable : false,\n       position : 4\n    }\n  ]\n}  From this you can see that the columns  film  and  competition  reference their eponymous tables. Let's ask the server for each film along with names of the competitions it entered. You don't have to do any custom coding. Send this query:  GET http://localhost:3000/film?select=title,competition{name}  [\n  {\n     title :  Chuang ru zhe ,\n     competition : [\n      {\n         name :  Golden Lion \n      }\n    ]\n  },\n  {\n     title :  The Look of Silence ,\n     competition : [\n      {\n         name :  Golden Lion \n      }\n    ]\n  },\n  ...\n]  The relation flows both ways. Here is how to get the name of each competition's name and the movies shown at it.  GET http://localhost:3000/competition?select=name,film{title}  [\n  {\n     name :  Golden Lion ,\n     film : [\n      {\n         title :  Chuang ru zhe \n      },\n      {\n         title :  The Look of Silence \n      },\n      ...\n    ]\n  },\n  {\n     name :  Palme d'Or ,\n     film : [\n      {\n         title :  The Wonders \n      },\n      {\n         title :  Foxcatcher \n      },\n      ...\n    ]\n  }\n]  Why not learn about the directors too? There is a many-to-one relation directly between films and directors. We can alter our previous query to include directors in its results.  GET http://localhost:3000/competition?select=name,film{title,director{*}}  [\n  {\n     name :  Golden Lion ,\n     film : [\n      {\n         title :  Manglehorn ,\n         director : {\n           name :  David Gordon Green \n        }\n      },\n      {\n         title :  Belye nochi pochtalona Alekseya Tryapitsyna ,\n         director : {\n           name :  Andrey Konchalovskiy \n        }\n      },\n      ...\n    ]\n  },\n  ...\n]", 
            "title": "Getting and Embedding Data"
        }, 
        {
            "location": "/examples/start/#singular-responses", 
            "text": "How do we ask for a single film, for instance the second one we inserted?  GET http://localhost:3000/film?id=eq.2  It returns  [\n  {\n     id : 2,\n     title :  The Look of Silence ,\n     year :  2014-01-01 ,\n     director :  Joshua Oppenheimer ,\n     rating : 8.3,\n     language :  Indonesian \n  }\n]  Like any query, it gives us a result  set , in this case an array with one element. However you and I know that  id  is a primary key, it will never return more than one result. We might want it returned as a JSON object, not an array. To express this preference include the header  Prefer: plurality=singular . It will respond with  {\n   id : 2,\n   title :  The Look of Silence ,\n   year :  2014-01-01 ,\n   director :  Joshua Oppenheimer ,\n   rating : 8.3,\n   language :  Indonesian \n}  \n     Why this approach to singular responses? \n\n     \n    PostgREST knows which columns comprise a primary key for a\n    table, so why not automatically choose plurality=singular when\n    these column filters are present? The fact is it could come as a\n    shock to a client that by adding one more filter condition it can\n    change the entire response format.\n     \n     \n    Then why not expose another kind of route such as /film/2 to indicate\n    one particular film? Because this does not accommodate compound keys.\n    The convention complects a plurality preference with table key\n    assumptions. We should separate concerns.\n     \n     \n    It turns out you can still have routes like /film/2.  Use a\n    proxy such as Nginx. It can rewrite routes such as /films/2\n    into /films?id=eq.2 and add the Prefer header to make the results\n    singular.", 
            "title": "Singular Responses"
        }, 
        {
            "location": "/examples/start/#conclusion", 
            "text": "This tutorial showed how to create a database with a basic schema, run PostgREST, and interact with the API. The next tutorial will show how to enable security for a multi-tenant blogging API.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/examples/users/", 
            "text": "User Management\n\n\nAPI clients authenticate with \nJSON Web Tokens\n.\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na username and password system on top of JWT using only plpgsql.\n\n\nFuture examples such as the multi-tenant blogging platform will use\nthe results from this example for their auth. We will build a system\nfor users to sign up, log in, manage their accounts, and for admins\nto manage other people's accounts. We will also see how to trigger\noutside events like sending password reset emails.\n\n\nBefore jumping into the code, a little more about how the tokens\nwork. Every JWT contains cryptographically signed \nclaims\n. PostgREST\ncares specifically about a claim called \nrole\n. When a client includes\na \nrole\n claim PostgREST executes their request using that database\nrole.\n\n\nHow would a client include a role claim, or claims in general?\nWithout knowing the server JWT secret a client cannot create a\nclaim. The only place to get a JWT is from the PostgREST server or\nfrom another service sharing the secret and acting on its behalf.\nWe'll use a stored procedure returning type \njwt_claims\n which is\na special type causing the server to encrypt and sign the return\nvalue.\n\n\nStoring Users and Passwords\n\n\nWe create a database schema especially for auth information. We'll\nalso need the postgres extension\n\npgcrypto\n.\n\n\ncreate extension if not exists pgcrypto;\n\n-- We put things inside the basic_auth schema to hide\n-- them from public view. Certain public procs/views will\n-- refer to helpers and tables inside.\ncreate schema if not exists basic_auth;\n\n\n\n\nNext a table to store the mapping from usernames and passwords to\ndatabase roles. The code below includes triggers and functions to\nencrypt the password and ensure the role exists.\n\n\ncreate table if not exists\nbasic_auth.users (\n  email    text primary key check ( email ~* '^.+@.+\\..+$' ),\n  pass     text not null check (length(pass) \n 512),\n  role     name not null check (length(role) \n 512),\n  verified boolean not null default false\n  -- If you like add more columns, or a json column\n);\n\ncreate or replace function\nbasic_auth.check_role_exists() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if not exists (select 1 from pg_roles as r where r.rolname = new.role) then\n    raise foreign_key_violation using message =\n      'unknown database role: ' || new.role;\n    return null;\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists ensure_user_role_exists on basic_auth.users;\ncreate constraint trigger ensure_user_role_exists\n  after insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.check_role_exists();\n\ncreate or replace function\nbasic_auth.encrypt_pass() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if tg_op = 'INSERT' or new.pass \n old.pass then\n    new.pass = crypt(new.pass, gen_salt('bf'));\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists encrypt_pass on basic_auth.users;\ncreate trigger encrypt_pass\n  before insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.encrypt_pass();\n\n\n\n\nWith the table in place we can make a helper to check passwords.\nIt returns the database role for a user if the email and password\nare correct.\n\n\ncreate or replace function\nbasic_auth.user_role(email text, pass text) returns name\n  language plpgsql\n  as $$\nbegin\n  return (\n  select role from basic_auth.users\n   where users.email = user_role.email\n     and users.pass = crypt(user_role.pass, users.pass)\n  );\nend;\n$$;\n\n\n\n\nPassword Reset\n\n\nWhen a user requests a password reset or signs up we create a token\nthey will use later to prove their identity. The tokens go in this\ntable.\n\n\ndrop type if exists token_type_enum cascade;\ncreate type token_type_enum as enum ('validation', 'reset');\n\ncreate table if not exists\nbasic_auth.tokens (\n  token       uuid primary key,\n  token_type  token_type_enum not null,\n  email       text not null references basic_auth.users (email)\n                on delete cascade on update cascade,\n  created_at  timestamptz not null default current_date\n);\n\n\n\n\nIn the main schema (as opposed to the \nbasic_auth\n schema) we expose\na password reset request function. HTTP clients will call it. The\nfunction takes the email address of the user.\n\n\ncreate or replace function\nrequest_password_reset(email text) returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = request_password_reset.email;\n\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', request_password_reset.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', request_password_reset.email,\n      'token', tok,\n      'token_type', 'reset'\n    )::text\n  );\nend;\n$$;\n\n\n\n\nThis function does not send any emails. It sends a postgres\n\nNOTIFY\n\ncommand. External programs such as a mailer listen for this event\nand do the work. The most robust way to process these signals is\nby pushing them onto work queues. Here are two programs to do that:\n\n\n\n\naweber/pgsql-listen-exchange\n for RabbitMQ\n\n\nSpiderOak/skeeter\n for ZeroMQ\n\n\n\n\nFor experimentation you don't need that though. Here's a sample\nNode program that listens for the events and logs them to stdout.\n\n\nvar PS = require('pg-pubsub');\n\nif(process.argv.length !== 3) {\n  console.log(\nUSAGE: DB_URL\n);\n  process.exit(2);\n}\nvar url  = process.argv[2],\n    ps   = new PS(url);\n\n// password reset request events\nps.addChannel('reset', console.log);\n// email validation required event\nps.addChannel('validate', console.log);\n\n// modify me to send emails\n\n\n\n\nOnce the user has a reset token they can use it as an argument to\nthe password reset function, calling it through the PostgREST RPC\ninterface.\n\n\ncreate or replace function\nreset_password(email text, token uuid, pass text)\n  returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  if exists(select 1 from basic_auth.tokens\n             where tokens.email = reset_password.email\n               and tokens.token = reset_password.token\n               and token_type = 'reset') then\n    update basic_auth.users set pass=reset_password.pass\n     where users.email = reset_password.email;\n\n    delete from basic_auth.tokens\n     where tokens.email = reset_password.email\n       and tokens.token = reset_password.token\n       and token_type = 'reset';\n  else\n    raise invalid_password using message =\n      'invalid user or token';\n  end if;\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = reset_password.email;\n\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', reset_password.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', reset_password.email,\n      'token', tok\n    )::text\n  );\nend;\n$$;\n\n\n\n\nEmail Validation\n\n\nThis is similar to password resets. Once again we generate a token.\nIt differs in that there is a trigger to send validations when a\nnew login is added to the users table.\n\n\ncreate or replace function\nbasic_auth.send_validation() returns trigger\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'validation', new.email);\n  perform pg_notify('validate',\n    json_build_object(\n      'email', new.email,\n      'token', tok,\n      'token_type', 'validation'\n    )::text\n  );\n  return new;\nend\n$$;\n\ndrop trigger if exists send_validation on basic_auth.users;\ncreate trigger send_validation\n  after insert on basic_auth.users\n  for each row\n  execute procedure basic_auth.send_validation();\n\n\n\n\nEditing Own User\n\n\nWe'll construct a redacted view for users. It hides passwords and\nshows only those users whose roles the currently logged in user has\ndb permission to access.\n\n\ncreate or replace view users as\nselect actual.role as role,\n       '***'::text as pass,\n       actual.email as email,\n       actual.verified as verified\nfrom basic_auth.users as actual,\n     (select rolname\n        from pg_authid\n       where pg_has_role(current_user, oid, 'member')\n     ) as member_of\nwhere actual.role = member_of.rolname;\n  -- can also add restriction that current_setting('postgrest.claims.email')\n  -- is equal to email so that user can only see themselves\n\n\n\n\nUsing this view clients can see themselves and any other users with\nthe right db roles. This view does not yet support inserts or updates\nbecause not all the columns refer directly to underlying columns.\nNor do we want it to be auto-updatable because it would allow an escalation\nof privileges. Someone could update their own row and change their\nrole to become more powerful.\n\n\nWe'll handle updates with a trigger, but we'll need a helper function\nto prevent an escalation of privileges.\n\n\ncreate or replace function\nbasic_auth.clearance_for_role(u name) returns void as\n$$\ndeclare\n  ok boolean;\nbegin\n  select exists (\n    select rolname\n      from pg_authid\n     where pg_has_role(current_user, oid, 'member')\n       and rolname = u\n  ) into ok;\n  if not ok then\n    raise invalid_password using message =\n      'current user not member of role ' || u;\n  end if;\nend\n$$ LANGUAGE plpgsql;\n\n\n\n\nWith the above function we can now make a safe trigger to allow\nuser updates.\n\n\ncreate or replace function\nupdate_users() returns trigger\nlanguage plpgsql\nAS $$\nbegin\n  if tg_op = 'INSERT' then\n    perform basic_auth.clearance_for_role(new.role);\n\n    insert into basic_auth.users\n      (role, pass, email, verified)\n    values\n      (new.role, new.pass, new.email,\n      coalesce(new.verified, false));\n    return new;\n  elsif tg_op = 'UPDATE' then\n    -- no need to check clearance for old.role because\n    -- an ineligible row would not have been available to update (http 404)\n    perform basic_auth.clearance_for_role(new.role);\n\n    update basic_auth.users set\n      email  = new.email,\n      role   = new.role,\n      pass   = new.pass,\n      verified = coalesce(new.verified, old.verified, false)\n      where email = old.email;\n    return new;\n  elsif tg_op = 'DELETE' then\n    -- no need to check clearance for old.role (see previous case)\n\n    delete from basic_auth.users\n     where basic_auth.email = old.email;\n    return null;\n  end if;\nend\n$$;\n\ndrop trigger if exists update_users on users;\ncreate trigger update_users\n  instead of insert or update or delete on\n    users for each row execute procedure update_users();\n\n\n\n\nFinally add a public function people can use to sign up. You can\nhard code a default db role in it. It alters the underlying\n\nbasic_auth.users\n so you can set whatever role you want without\nrestriction.\n\n\ncreate or replace function\nsignup(email text, pass text) returns void\nas $$\n  insert into basic_auth.users (email, pass, role) values\n    (signup.email, signup.pass, 'hardcoded-role-here');\n$$ language sql;\n\n\n\n\nGenerating JWT\n\n\nAs mentioned at the start, clients authenticate with JWT. PostgREST\nhas a special convention to allow your sql functions to return JWT.\nAny function that returns a type whose name ends in \njwt_claims\n will\nhave its return value encoded. For instance, let's make a login function\nwhich consults our users table.\n\n\nFirst create a return type:\n\n\ndrop type if exists basic_auth.jwt_claims cascade;\ncreate type basic_auth.jwt_claims AS (role text, email text);\n\n\n\n\nAnd now the function:\n\n\ncreate or replace function\nlogin(email text, pass text) returns basic_auth.jwt_claims\n  language plpgsql\n  as $$\ndeclare\n  _role name;\n  _verified boolean;\n  _email text;\n  result basic_auth.jwt_claims;\nbegin\n  -- check email and password\n  select basic_auth.user_role(email, pass) into _role;\n  if _role is null then\n    raise invalid_password using message = 'invalid user or password';\n  end if;\n  -- check verified flag whether users\n  -- have validated their emails\n  _email := email;\n  select verified from basic_auth.users as u where u.email=_email limit 1 into _verified;\n  if not _verified then\n    raise invalid_authorization_specification using message = 'user is not verified';\n  end if;\n  select _role as role, login.email as email into result;\n  return result;\nend;\n$$;\n\n\n\n\nAn API request to login would look like this.\n\n\nPOST /rpc/login\n\n{ \nemail\n: \nfoo@bar.com\n, \npass\n: \nfoobar\n }\n\n\n\n\nResponse\n\n\n{\n  \ntoken\n: \neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto\n\n}\n\n\n\n\nTry decoding the token at \njwt.io\n. (It was encoded\nwith a secret of \nsecret\n which is the default.) To use this token\nin a future API request include it in an \nAuthorization\n request\nheader.\n\n\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto\n\n\n\n\nSame-Role Users\n\n\nYou may not want a separate db role for every user. You can distinguish\none user from another in SQL by examining the JWT claims which\nPostgREST makes available in the SQL variable \npostgrest.claims\n.\nHere's a function to get the email of the currently authenticated\nuser.\n\n\n-- Prevent current_setting('postgrest.claims.email') from raising\n-- an exception if the setting is not present. Default it to ''.\nALTER DATABASE your_db_name SET postgrest.claims.email TO '';\n\ncreate or replace function\nbasic_auth.current_email() returns text\n  language plpgsql\n  as $$\nbegin\n  return current_setting('postgrest.claims.email');\nend;\n$$;\n\n\n\n\nRemember that the \nlogin\n function set the claims \nemail\n and \nrole\n.\nYou can modify \nlogin\n to set other claims as well if they are\nuseful for your other SQL functions to reference later.\n\n\nPermissions\n\n\nBasic table-level permissions. We'll add an the \nauthenticator\n\nrole which can't do anything itself other than switch into other\nroles as directed by JWT.\n\n\ncreate role anon;\ncreate role authenticator noinherit;\ngrant anon to authenticator;\n\ngrant usage on schema public, basic_auth to anon;\n\n-- anon can create new logins\ngrant insert on table basic_auth.users, basic_auth.tokens to anon;\ngrant select on table pg_authid, basic_auth.users to anon;\ngrant execute on function\n  login(text,text),\n  request_password_reset(text),\n  reset_password(text,uuid,text),\n  signup(text, text)\n  to anon;\n\n\n\n\nConclusion\n\n\nThis section explained the implementation details for building a\npassword based authentication system in pure sql. The next example\nwill put it to work in a multi-tenant blogging API.", 
            "title": "User Management"
        }, 
        {
            "location": "/examples/users/#user-management", 
            "text": "API clients authenticate with  JSON Web Tokens .\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na username and password system on top of JWT using only plpgsql.  Future examples such as the multi-tenant blogging platform will use\nthe results from this example for their auth. We will build a system\nfor users to sign up, log in, manage their accounts, and for admins\nto manage other people's accounts. We will also see how to trigger\noutside events like sending password reset emails.  Before jumping into the code, a little more about how the tokens\nwork. Every JWT contains cryptographically signed  claims . PostgREST\ncares specifically about a claim called  role . When a client includes\na  role  claim PostgREST executes their request using that database\nrole.  How would a client include a role claim, or claims in general?\nWithout knowing the server JWT secret a client cannot create a\nclaim. The only place to get a JWT is from the PostgREST server or\nfrom another service sharing the secret and acting on its behalf.\nWe'll use a stored procedure returning type  jwt_claims  which is\na special type causing the server to encrypt and sign the return\nvalue.", 
            "title": "User Management"
        }, 
        {
            "location": "/examples/users/#storing-users-and-passwords", 
            "text": "We create a database schema especially for auth information. We'll\nalso need the postgres extension pgcrypto .  create extension if not exists pgcrypto;\n\n-- We put things inside the basic_auth schema to hide\n-- them from public view. Certain public procs/views will\n-- refer to helpers and tables inside.\ncreate schema if not exists basic_auth;  Next a table to store the mapping from usernames and passwords to\ndatabase roles. The code below includes triggers and functions to\nencrypt the password and ensure the role exists.  create table if not exists\nbasic_auth.users (\n  email    text primary key check ( email ~* '^.+@.+\\..+$' ),\n  pass     text not null check (length(pass)   512),\n  role     name not null check (length(role)   512),\n  verified boolean not null default false\n  -- If you like add more columns, or a json column\n);\n\ncreate or replace function\nbasic_auth.check_role_exists() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if not exists (select 1 from pg_roles as r where r.rolname = new.role) then\n    raise foreign_key_violation using message =\n      'unknown database role: ' || new.role;\n    return null;\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists ensure_user_role_exists on basic_auth.users;\ncreate constraint trigger ensure_user_role_exists\n  after insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.check_role_exists();\n\ncreate or replace function\nbasic_auth.encrypt_pass() returns trigger\n  language plpgsql\n  as $$\nbegin\n  if tg_op = 'INSERT' or new.pass   old.pass then\n    new.pass = crypt(new.pass, gen_salt('bf'));\n  end if;\n  return new;\nend\n$$;\n\ndrop trigger if exists encrypt_pass on basic_auth.users;\ncreate trigger encrypt_pass\n  before insert or update on basic_auth.users\n  for each row\n  execute procedure basic_auth.encrypt_pass();  With the table in place we can make a helper to check passwords.\nIt returns the database role for a user if the email and password\nare correct.  create or replace function\nbasic_auth.user_role(email text, pass text) returns name\n  language plpgsql\n  as $$\nbegin\n  return (\n  select role from basic_auth.users\n   where users.email = user_role.email\n     and users.pass = crypt(user_role.pass, users.pass)\n  );\nend;\n$$;", 
            "title": "Storing Users and Passwords"
        }, 
        {
            "location": "/examples/users/#password-reset", 
            "text": "When a user requests a password reset or signs up we create a token\nthey will use later to prove their identity. The tokens go in this\ntable.  drop type if exists token_type_enum cascade;\ncreate type token_type_enum as enum ('validation', 'reset');\n\ncreate table if not exists\nbasic_auth.tokens (\n  token       uuid primary key,\n  token_type  token_type_enum not null,\n  email       text not null references basic_auth.users (email)\n                on delete cascade on update cascade,\n  created_at  timestamptz not null default current_date\n);  In the main schema (as opposed to the  basic_auth  schema) we expose\na password reset request function. HTTP clients will call it. The\nfunction takes the email address of the user.  create or replace function\nrequest_password_reset(email text) returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = request_password_reset.email;\n\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', request_password_reset.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', request_password_reset.email,\n      'token', tok,\n      'token_type', 'reset'\n    )::text\n  );\nend;\n$$;  This function does not send any emails. It sends a postgres NOTIFY \ncommand. External programs such as a mailer listen for this event\nand do the work. The most robust way to process these signals is\nby pushing them onto work queues. Here are two programs to do that:   aweber/pgsql-listen-exchange  for RabbitMQ  SpiderOak/skeeter  for ZeroMQ   For experimentation you don't need that though. Here's a sample\nNode program that listens for the events and logs them to stdout.  var PS = require('pg-pubsub');\n\nif(process.argv.length !== 3) {\n  console.log( USAGE: DB_URL );\n  process.exit(2);\n}\nvar url  = process.argv[2],\n    ps   = new PS(url);\n\n// password reset request events\nps.addChannel('reset', console.log);\n// email validation required event\nps.addChannel('validate', console.log);\n\n// modify me to send emails  Once the user has a reset token they can use it as an argument to\nthe password reset function, calling it through the PostgREST RPC\ninterface.  create or replace function\nreset_password(email text, token uuid, pass text)\n  returns void\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  if exists(select 1 from basic_auth.tokens\n             where tokens.email = reset_password.email\n               and tokens.token = reset_password.token\n               and token_type = 'reset') then\n    update basic_auth.users set pass=reset_password.pass\n     where users.email = reset_password.email;\n\n    delete from basic_auth.tokens\n     where tokens.email = reset_password.email\n       and tokens.token = reset_password.token\n       and token_type = 'reset';\n  else\n    raise invalid_password using message =\n      'invalid user or token';\n  end if;\n  delete from basic_auth.tokens\n   where token_type = 'reset'\n     and tokens.email = reset_password.email;\n\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'reset', reset_password.email);\n  perform pg_notify('reset',\n    json_build_object(\n      'email', reset_password.email,\n      'token', tok\n    )::text\n  );\nend;\n$$;", 
            "title": "Password Reset"
        }, 
        {
            "location": "/examples/users/#email-validation", 
            "text": "This is similar to password resets. Once again we generate a token.\nIt differs in that there is a trigger to send validations when a\nnew login is added to the users table.  create or replace function\nbasic_auth.send_validation() returns trigger\n  language plpgsql\n  as $$\ndeclare\n  tok uuid;\nbegin\n  select gen_random_uuid() into tok;\n  insert into basic_auth.tokens (token, token_type, email)\n         values (tok, 'validation', new.email);\n  perform pg_notify('validate',\n    json_build_object(\n      'email', new.email,\n      'token', tok,\n      'token_type', 'validation'\n    )::text\n  );\n  return new;\nend\n$$;\n\ndrop trigger if exists send_validation on basic_auth.users;\ncreate trigger send_validation\n  after insert on basic_auth.users\n  for each row\n  execute procedure basic_auth.send_validation();", 
            "title": "Email Validation"
        }, 
        {
            "location": "/examples/users/#editing-own-user", 
            "text": "We'll construct a redacted view for users. It hides passwords and\nshows only those users whose roles the currently logged in user has\ndb permission to access.  create or replace view users as\nselect actual.role as role,\n       '***'::text as pass,\n       actual.email as email,\n       actual.verified as verified\nfrom basic_auth.users as actual,\n     (select rolname\n        from pg_authid\n       where pg_has_role(current_user, oid, 'member')\n     ) as member_of\nwhere actual.role = member_of.rolname;\n  -- can also add restriction that current_setting('postgrest.claims.email')\n  -- is equal to email so that user can only see themselves  Using this view clients can see themselves and any other users with\nthe right db roles. This view does not yet support inserts or updates\nbecause not all the columns refer directly to underlying columns.\nNor do we want it to be auto-updatable because it would allow an escalation\nof privileges. Someone could update their own row and change their\nrole to become more powerful.  We'll handle updates with a trigger, but we'll need a helper function\nto prevent an escalation of privileges.  create or replace function\nbasic_auth.clearance_for_role(u name) returns void as\n$$\ndeclare\n  ok boolean;\nbegin\n  select exists (\n    select rolname\n      from pg_authid\n     where pg_has_role(current_user, oid, 'member')\n       and rolname = u\n  ) into ok;\n  if not ok then\n    raise invalid_password using message =\n      'current user not member of role ' || u;\n  end if;\nend\n$$ LANGUAGE plpgsql;  With the above function we can now make a safe trigger to allow\nuser updates.  create or replace function\nupdate_users() returns trigger\nlanguage plpgsql\nAS $$\nbegin\n  if tg_op = 'INSERT' then\n    perform basic_auth.clearance_for_role(new.role);\n\n    insert into basic_auth.users\n      (role, pass, email, verified)\n    values\n      (new.role, new.pass, new.email,\n      coalesce(new.verified, false));\n    return new;\n  elsif tg_op = 'UPDATE' then\n    -- no need to check clearance for old.role because\n    -- an ineligible row would not have been available to update (http 404)\n    perform basic_auth.clearance_for_role(new.role);\n\n    update basic_auth.users set\n      email  = new.email,\n      role   = new.role,\n      pass   = new.pass,\n      verified = coalesce(new.verified, old.verified, false)\n      where email = old.email;\n    return new;\n  elsif tg_op = 'DELETE' then\n    -- no need to check clearance for old.role (see previous case)\n\n    delete from basic_auth.users\n     where basic_auth.email = old.email;\n    return null;\n  end if;\nend\n$$;\n\ndrop trigger if exists update_users on users;\ncreate trigger update_users\n  instead of insert or update or delete on\n    users for each row execute procedure update_users();  Finally add a public function people can use to sign up. You can\nhard code a default db role in it. It alters the underlying basic_auth.users  so you can set whatever role you want without\nrestriction.  create or replace function\nsignup(email text, pass text) returns void\nas $$\n  insert into basic_auth.users (email, pass, role) values\n    (signup.email, signup.pass, 'hardcoded-role-here');\n$$ language sql;", 
            "title": "Editing Own User"
        }, 
        {
            "location": "/examples/users/#generating-jwt", 
            "text": "As mentioned at the start, clients authenticate with JWT. PostgREST\nhas a special convention to allow your sql functions to return JWT.\nAny function that returns a type whose name ends in  jwt_claims  will\nhave its return value encoded. For instance, let's make a login function\nwhich consults our users table.  First create a return type:  drop type if exists basic_auth.jwt_claims cascade;\ncreate type basic_auth.jwt_claims AS (role text, email text);  And now the function:  create or replace function\nlogin(email text, pass text) returns basic_auth.jwt_claims\n  language plpgsql\n  as $$\ndeclare\n  _role name;\n  _verified boolean;\n  _email text;\n  result basic_auth.jwt_claims;\nbegin\n  -- check email and password\n  select basic_auth.user_role(email, pass) into _role;\n  if _role is null then\n    raise invalid_password using message = 'invalid user or password';\n  end if;\n  -- check verified flag whether users\n  -- have validated their emails\n  _email := email;\n  select verified from basic_auth.users as u where u.email=_email limit 1 into _verified;\n  if not _verified then\n    raise invalid_authorization_specification using message = 'user is not verified';\n  end if;\n  select _role as role, login.email as email into result;\n  return result;\nend;\n$$;  An API request to login would look like this.  POST /rpc/login\n\n{  email :  foo@bar.com ,  pass :  foobar  }  Response  {\n   token :  eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto \n}  Try decoding the token at  jwt.io . (It was encoded\nwith a secret of  secret  which is the default.) To use this token\nin a future API request include it in an  Authorization  request\nheader.  Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImZvb0BiYXIuY29tIiwicm9sZSI6ImF1dGhvciJ9.KHwYdK9dAMAg-MGCQXuDiFuvbmW-y8FjfYIcMrETnto", 
            "title": "Generating JWT"
        }, 
        {
            "location": "/examples/users/#same-role-users", 
            "text": "You may not want a separate db role for every user. You can distinguish\none user from another in SQL by examining the JWT claims which\nPostgREST makes available in the SQL variable  postgrest.claims .\nHere's a function to get the email of the currently authenticated\nuser.  -- Prevent current_setting('postgrest.claims.email') from raising\n-- an exception if the setting is not present. Default it to ''.\nALTER DATABASE your_db_name SET postgrest.claims.email TO '';\n\ncreate or replace function\nbasic_auth.current_email() returns text\n  language plpgsql\n  as $$\nbegin\n  return current_setting('postgrest.claims.email');\nend;\n$$;  Remember that the  login  function set the claims  email  and  role .\nYou can modify  login  to set other claims as well if they are\nuseful for your other SQL functions to reference later.", 
            "title": "Same-Role Users"
        }, 
        {
            "location": "/examples/users/#permissions", 
            "text": "Basic table-level permissions. We'll add an the  authenticator \nrole which can't do anything itself other than switch into other\nroles as directed by JWT.  create role anon;\ncreate role authenticator noinherit;\ngrant anon to authenticator;\n\ngrant usage on schema public, basic_auth to anon;\n\n-- anon can create new logins\ngrant insert on table basic_auth.users, basic_auth.tokens to anon;\ngrant select on table pg_authid, basic_auth.users to anon;\ngrant execute on function\n  login(text,text),\n  request_password_reset(text),\n  reset_password(text,uuid,text),\n  signup(text, text)\n  to anon;", 
            "title": "Permissions"
        }, 
        {
            "location": "/examples/users/#conclusion", 
            "text": "This section explained the implementation details for building a\npassword based authentication system in pure sql. The next example\nwill put it to work in a multi-tenant blogging API.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/examples/blog/", 
            "text": "Multi-Tenant Blog\n\n\nIn our blog app there will be anonymous users and authors. Each\nauthor can create and edit their own posts, and read (but not edit)\nthe posts of other authors. Anonymous users cannot edit anything\nbut can sign up for author accounts. Authors can also post comments\non articles.\n\n\nThis example builds off the previous previous \nUser Management\n\none. We had previously created a signup and login system on top of\nJWT. We'll use this auth system for the blog. \nRun the SQL in the\nprevious example\n first, before continuing with this example.\n\n\nFor your convenience, the complete sql for the blog demo is\n\nhere\n.\nYou can try it out in this \nvagrant\nimage\n as well.\n\n\nAdding Blog-Specific Tables\n\n\nStoring the posts and comments is this simple. The comments do not\nform a tree, they are linear under a post.\n\n\ncreate table if not exists\nposts (\n  id         bigserial primary key,\n  title      text not null,\n  body       text not null,\n  author     text not null references basic_auth.users (email)\n               on delete restrict on update cascade\n               default basic_auth.current_email(),\n  created_at timestamptz not null default current_date\n);\n\ncreate table if not exists\ncomments (\n  id         bigserial primary key,\n  body       text not null,\n  author     text not null references basic_auth.users (email)\n               on delete restrict on update cascade\n               default basic_auth.current_email(),\n  post       bigint not null references posts (id)\n               on delete cascade on update cascade,\n  created_at timestamptz not null default current_date\n);\n\n\n\n\nPermissions\n\n\nOn top of the \nauthenticator\n and \nanon\n access granted in the\nprevious example, blogs have an \nauthor\n role with extra permissions.\n\n\ncreate role author;\ngrant author to authenticator;\n\ngrant usage on schema public, basic_auth to author;\n\n-- authors can edit comments/posts\ngrant select, insert, update, delete\n  on basic_auth.tokens, basic_auth.users to author;\ngrant select, insert, update, delete\n  on table users, posts, comments to author;\ngrant usage, select on sequence posts_id_seq, comments_id_seq to author;\n\n\n\n\nTo ensure that authors cannot edit each others' posts and comments\nwe'll use \nrow-level\nsecurity\n.\nNote that it requires PostgreSQL 9.5 or later.\n\n\ngrant select on posts, comments to anon;\n\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\nALTER TABLE comments ENABLE ROW LEVEL SECURITY;\n\ndrop policy if exists posts_select_unsecure on posts;\ncreate policy posts_select_unsecure on posts for select\n  using (true);\n\ndrop policy if exists comments_select_unsecure on comments;\ncreate policy comments_select_unsecure on comments for select\n  using (true);\n\ndrop policy if exists authors_eigencreate on posts;\ncreate policy authors_eigencreate on posts for insert\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigencreate on comments;\ncreate policy authors_eigencreate on comments for insert\n  with check (\n      author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigenedit on posts;\ncreate policy authors_eigenedit on posts for update\n  using (author = basic_auth.current_email())\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigenedit on comments;\ncreate policy authors_eigenedit on comments for update\n  using (author = basic_auth.current_email())\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigendelete on posts;\ncreate policy authors_eigendelete on posts for delete\n  using (author = basic_auth.current_email());\n\ndrop policy if exists authors_eigendelete on comments;\ncreate policy authors_eigendelete on comments for delete\n  using (author = basic_auth.current_email());\n\n\n\n\nFinally we need to modify the \nusers\n view from the previous example.\nThis is because all authors share a single db role. We could have\nchosen to assign a new role for every author (all inheriting from\n\nauthor\n) but we choose to tell them apart by their email addresses.\nThe addition below prevents authors from seeing each others' info\nin the \nusers\n view.\n\n\n  create or replace view users as\n  select actual.role as role,\n         '***'::text as pass,\n         actual.email as email,\n         actual.verified as verified\n  from basic_auth.users as actual,\n       (select rolname\n          from pg_authid\n         where pg_has_role(current_user, oid, 'member')\n       ) as member_of\n  where actual.role = member_of.rolname\n+   and (\n+     actual.role \n 'author'\n+     or email = basic_auth.current_email()\n+   );\n\n\n\n\nExample client queries\n\n\n\n\nTop ten most recent posts\n\n\n\n\n  GET /posts?order=created_at.desc\n  Range: 0-9\n\n\n\n\n\n\nSingle post (randomly chose id=1) with its comments\n\n\n\n\n  GET /posts?id=eq.1\nselect=*,comments{*}\n\n\n\n\n\n\nAdd a new post\n\n\n\n\n  POST /posts\n  Authorization: Bearer [JWT TOKEN]\n\n  {\n    \ntitle\n: \nMy first post\n,\n    \nbody\n: \nMeh, forgot what I wanted to say.\n\n  }\n\n\n\n\nConclusion\n\n\nVoil\u00e0, a blog API. Most of the code ended up being for defining\nsecurity. Once you have set up an authentication system, the code\nto do application specific things like blog posts and comments is\nshort.  All the front-end routes and verbs are created automatically\nfor you.", 
            "title": "Multi-Tenant Blog"
        }, 
        {
            "location": "/examples/blog/#multi-tenant-blog", 
            "text": "In our blog app there will be anonymous users and authors. Each\nauthor can create and edit their own posts, and read (but not edit)\nthe posts of other authors. Anonymous users cannot edit anything\nbut can sign up for author accounts. Authors can also post comments\non articles.  This example builds off the previous previous  User Management \none. We had previously created a signup and login system on top of\nJWT. We'll use this auth system for the blog.  Run the SQL in the\nprevious example  first, before continuing with this example.  For your convenience, the complete sql for the blog demo is here .\nYou can try it out in this  vagrant\nimage  as well.", 
            "title": "Multi-Tenant Blog"
        }, 
        {
            "location": "/examples/blog/#adding-blog-specific-tables", 
            "text": "Storing the posts and comments is this simple. The comments do not\nform a tree, they are linear under a post.  create table if not exists\nposts (\n  id         bigserial primary key,\n  title      text not null,\n  body       text not null,\n  author     text not null references basic_auth.users (email)\n               on delete restrict on update cascade\n               default basic_auth.current_email(),\n  created_at timestamptz not null default current_date\n);\n\ncreate table if not exists\ncomments (\n  id         bigserial primary key,\n  body       text not null,\n  author     text not null references basic_auth.users (email)\n               on delete restrict on update cascade\n               default basic_auth.current_email(),\n  post       bigint not null references posts (id)\n               on delete cascade on update cascade,\n  created_at timestamptz not null default current_date\n);", 
            "title": "Adding Blog-Specific Tables"
        }, 
        {
            "location": "/examples/blog/#permissions", 
            "text": "On top of the  authenticator  and  anon  access granted in the\nprevious example, blogs have an  author  role with extra permissions.  create role author;\ngrant author to authenticator;\n\ngrant usage on schema public, basic_auth to author;\n\n-- authors can edit comments/posts\ngrant select, insert, update, delete\n  on basic_auth.tokens, basic_auth.users to author;\ngrant select, insert, update, delete\n  on table users, posts, comments to author;\ngrant usage, select on sequence posts_id_seq, comments_id_seq to author;  To ensure that authors cannot edit each others' posts and comments\nwe'll use  row-level\nsecurity .\nNote that it requires PostgreSQL 9.5 or later.  grant select on posts, comments to anon;\n\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\nALTER TABLE comments ENABLE ROW LEVEL SECURITY;\n\ndrop policy if exists posts_select_unsecure on posts;\ncreate policy posts_select_unsecure on posts for select\n  using (true);\n\ndrop policy if exists comments_select_unsecure on comments;\ncreate policy comments_select_unsecure on comments for select\n  using (true);\n\ndrop policy if exists authors_eigencreate on posts;\ncreate policy authors_eigencreate on posts for insert\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigencreate on comments;\ncreate policy authors_eigencreate on comments for insert\n  with check (\n      author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigenedit on posts;\ncreate policy authors_eigenedit on posts for update\n  using (author = basic_auth.current_email())\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigenedit on comments;\ncreate policy authors_eigenedit on comments for update\n  using (author = basic_auth.current_email())\n  with check (\n    author = basic_auth.current_email()\n  );\n\ndrop policy if exists authors_eigendelete on posts;\ncreate policy authors_eigendelete on posts for delete\n  using (author = basic_auth.current_email());\n\ndrop policy if exists authors_eigendelete on comments;\ncreate policy authors_eigendelete on comments for delete\n  using (author = basic_auth.current_email());  Finally we need to modify the  users  view from the previous example.\nThis is because all authors share a single db role. We could have\nchosen to assign a new role for every author (all inheriting from author ) but we choose to tell them apart by their email addresses.\nThe addition below prevents authors from seeing each others' info\nin the  users  view.    create or replace view users as\n  select actual.role as role,\n         '***'::text as pass,\n         actual.email as email,\n         actual.verified as verified\n  from basic_auth.users as actual,\n       (select rolname\n          from pg_authid\n         where pg_has_role(current_user, oid, 'member')\n       ) as member_of\n  where actual.role = member_of.rolname\n+   and (\n+     actual.role   'author'\n+     or email = basic_auth.current_email()\n+   );", 
            "title": "Permissions"
        }, 
        {
            "location": "/examples/blog/#example-client-queries", 
            "text": "Top ten most recent posts     GET /posts?order=created_at.desc\n  Range: 0-9   Single post (randomly chose id=1) with its comments     GET /posts?id=eq.1 select=*,comments{*}   Add a new post     POST /posts\n  Authorization: Bearer [JWT TOKEN]\n\n  {\n     title :  My first post ,\n     body :  Meh, forgot what I wanted to say. \n  }", 
            "title": "Example client queries"
        }, 
        {
            "location": "/examples/blog/#conclusion", 
            "text": "Voil\u00e0, a blog API. Most of the code ended up being for defining\nsecurity. Once you have set up an authentication system, the code\nto do application specific things like blog posts and comments is\nshort.  All the front-end routes and verbs are created automatically\nfor you.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/examples/external_auth/", 
            "text": "External Authentication\n\n\nAPI clients authenticate with \nJSON Web Tokens\n.\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na system that works with an external authentication server\nand integrates with a PostgREST server by sharing the same JWT secret.\n\n\nFor a better understanding of JWT and PostgREST authentication system you should read\nthe \nUser Management\n example as well.\n\n\nI'll use a \nRails\n application using \nDevise\n\njust to make the example more concrete, but this could be replicated for\nany other external authentication system using the same principles.\nIn case Rails is not your cup of tea you can continue reading and\njust skip the Ruby code samples. I'll also assume\nthe use of JQuery for some client-side code samples for the sake of simplicity.\n\n\nI won't delve into Devise authentication details, for this would require a tutorial on its own,\nso I'm assuming that the reader's authentication system is already working.\n\n\nSharing the JWT Secret\n\n\nAllowing a third party to generate valid JWTs for your PostgREST API\nis just a matter of sharing a secret. So you need to give your authenticator\nsoftware the same secret that was used in your API server under the \n--jwt-secret\n\nparameter.\n\n\nThis could be done easly using environment variables. You set a \nJWT_SECRET\n variable\nin the environment where you run your rails app and it will be accessible in the global\nvariable \nENV['JWT_SECRET']\n.\n\n\nUser Model\n\n\nWe will map each user in this example to two database roles.\nSo our application users are either \nadmin\n or \ncustomer\n.\nIf they are just visitors (not logged in) to our website they will be \nanonymous\n.\nOne way of mapping users is to add a field in our users table indicating their database role.\nI'll add a text field called role to my users table:\n\n\nALTER TABLE users ADD role text NOT NULL DEFAULT 'customer';\n\n\n\n\nBesides the main user that PostgREST uses to connect to PostgreSQL\nand the anonymous user, we will need two additional roles for our example:\n\n\n\n\nadmin - to be used by users that access all the system rows.\n\n\ncustomer - to be used when user has restricted access to database rows.\n\n\n\n\nBellow we have the commands to create all roles that will be used:\n\n\nCREATE USER authenticator NOINHERIT;\nCREATE ROLE anonymous;\nCREATE ROLE admin;\nCREATE ROLE customer;\n\nGRANT customer, admin, anonymous TO authenticator;\n\n\n\n\nGenerating a JWT\n\n\nSeveral libraries are available to generate JWT, you will find a very handy list in \ntheir website\n\nunder \nLibraries\n.\nTo continue our Rails example I'll use the ruby library \njson_web_token\n.\n\n\nIn order to make the gem available in my Rails project I add the following line to my Gemfile:\n\n\ngem 'json_web_token'\n\n\n\n\nThen we create a Rails controller to serve JWTs for my authenticated users.\nFor this I just open a file \napp/controllers/api_tokens_controller.rb\n with the content:\n\n\nclass ApiTokensController \n ApplicationController\n  TOKEN_TTL = 1.hour\n\n  def show\n    unless ENV['JWT_SECRET'].present?\n      return render json: {error: \nyou need to have JWT_SECRET configured to get an API token\n}, status: 500\n    end\n\n    unless current_user.present?\n      return render json: {error: \nonly authenticated users can request the API token\n}, status: 401\n    end\n\n    expires_in TOKEN_TTL, public: false\n    render json: {token: jwt}, status: 200\n  end\n\n  private\n  def jwt\n    JsonWebToken.sign(claims, key: ENV['JWT_SECRET'])\n  end\n\n  def claims\n    # This token will expire 1 hour after being issued\n    {\n      role: current_user.role,\n      user_id: current_user.id.to_s,\n      exp: (Time.now + TOKEN_TTL).to_i\n    }\n  end\nend\n\n\n\n\n\n    \nToken Time to Live\n\n    \n\n    In the code above we leverage the HTTP time based cache headers to expire the\n    endpoint cache at the same time as the token. In this example we have a token\n    that will be refresh one hour after its issuing time.\n    That's why both are based on the \nTOKEN_TTL\n constant.\n    \n\n\n\n\n\nWe also need to create a route in the \nconfig/routes.rb\n file:\n\n\nresource :api_token, only: [:show]\n\n\n\n\nNow, any authenticated user in our rails application can request an api_token making a GET\nrequest to \n/api_token\n. This endpoint will return a json object with one property\nwhose value is the token the API requests should use.\n\n\nOrders Endpoint\n\n\nHere is how to create a view to generate an endpoint \n/orders\n filtered by\nthe logged in user:\n\n\nALTER DATABASE mydb SET postgrest.claims.user_id TO '';\n\nCREATE OR REPLACE FUNCTION current_user_id()\nRETURNS integer\nSTABLE\nLANGUAGE SQL\nAS $$\n    SELECT nullif(current_setting('postgrest.claims.user_id'), '')::integer;\n$$;\n\nCREATE SCHEMA private;\n\nCREATE TABLE private.orders (\n    id serial primary key,\n    user_id int references users,\n    created_at timestamp not null default current_timestamp,\n    updated_at timestamp not null default current_timestamp\n);\n\nCREATE VIEW orders AS\nSELECT\n    id, user_id, created_at, updated_at\nFROM\n    private.orders o\nWHERE\n    current_user = 'admin' OR o.user_id = current_user_id();\n\n\n\n\n\n    \nDRY priviledge checking conditions\n\n    \n\n    You can encapsulate conditions that will be commonly used to check for privileges while reading a database row.\n    We used a function \ncurrent_user_id()\n but we could add more conditions to functions\n    as the system becomes more complex.\n\n    Remeber to mark your functions as \nSTABLE\n so that PostgreSQL can inline then while planning the query.\n    \n\n\n\n\n\nUsing the JWT\n\n\nNow whenever you are authenticated in your Rails application you can use some Javascript\n code to get the token and use it:\n\n\n$.getJSON('/api_token').done(function(data){\n    $.ajax('/orders', {'Authorization': 'Bearer ' + data.token}).done(function(data){\n        console.log('Visible Orders: ', data);\n    })\n}).fail(function(){\n    console.log('Error fetching API token');\n})\n\n\n\n\nWe could also store the token to avoid having to fetch it again in the same page.\n\n\nConclusion\n\n\nThis section explained the implementation details for building an\nexternal authentication system working with PostgREST.\nWith the previous \nUser Management\n example this should give a clearer\nidea of how to set up authentication for your API.", 
            "title": "External Authentication"
        }, 
        {
            "location": "/examples/external_auth/#external-authentication", 
            "text": "API clients authenticate with  JSON Web Tokens .\nPostgREST does not support any other authentication mechanism\ndirectly, but they can be built on top. In this demo we will build\na system that works with an external authentication server\nand integrates with a PostgREST server by sharing the same JWT secret.  For a better understanding of JWT and PostgREST authentication system you should read\nthe  User Management  example as well.  I'll use a  Rails  application using  Devise \njust to make the example more concrete, but this could be replicated for\nany other external authentication system using the same principles.\nIn case Rails is not your cup of tea you can continue reading and\njust skip the Ruby code samples. I'll also assume\nthe use of JQuery for some client-side code samples for the sake of simplicity.  I won't delve into Devise authentication details, for this would require a tutorial on its own,\nso I'm assuming that the reader's authentication system is already working.", 
            "title": "External Authentication"
        }, 
        {
            "location": "/examples/external_auth/#sharing-the-jwt-secret", 
            "text": "Allowing a third party to generate valid JWTs for your PostgREST API\nis just a matter of sharing a secret. So you need to give your authenticator\nsoftware the same secret that was used in your API server under the  --jwt-secret \nparameter.  This could be done easly using environment variables. You set a  JWT_SECRET  variable\nin the environment where you run your rails app and it will be accessible in the global\nvariable  ENV['JWT_SECRET'] .", 
            "title": "Sharing the JWT Secret"
        }, 
        {
            "location": "/examples/external_auth/#user-model", 
            "text": "We will map each user in this example to two database roles.\nSo our application users are either  admin  or  customer .\nIf they are just visitors (not logged in) to our website they will be  anonymous .\nOne way of mapping users is to add a field in our users table indicating their database role.\nI'll add a text field called role to my users table:  ALTER TABLE users ADD role text NOT NULL DEFAULT 'customer';  Besides the main user that PostgREST uses to connect to PostgreSQL\nand the anonymous user, we will need two additional roles for our example:   admin - to be used by users that access all the system rows.  customer - to be used when user has restricted access to database rows.   Bellow we have the commands to create all roles that will be used:  CREATE USER authenticator NOINHERIT;\nCREATE ROLE anonymous;\nCREATE ROLE admin;\nCREATE ROLE customer;\n\nGRANT customer, admin, anonymous TO authenticator;", 
            "title": "User Model"
        }, 
        {
            "location": "/examples/external_auth/#generating-a-jwt", 
            "text": "Several libraries are available to generate JWT, you will find a very handy list in  their website \nunder  Libraries .\nTo continue our Rails example I'll use the ruby library  json_web_token .  In order to make the gem available in my Rails project I add the following line to my Gemfile:  gem 'json_web_token'  Then we create a Rails controller to serve JWTs for my authenticated users.\nFor this I just open a file  app/controllers/api_tokens_controller.rb  with the content:  class ApiTokensController   ApplicationController\n  TOKEN_TTL = 1.hour\n\n  def show\n    unless ENV['JWT_SECRET'].present?\n      return render json: {error:  you need to have JWT_SECRET configured to get an API token }, status: 500\n    end\n\n    unless current_user.present?\n      return render json: {error:  only authenticated users can request the API token }, status: 401\n    end\n\n    expires_in TOKEN_TTL, public: false\n    render json: {token: jwt}, status: 200\n  end\n\n  private\n  def jwt\n    JsonWebToken.sign(claims, key: ENV['JWT_SECRET'])\n  end\n\n  def claims\n    # This token will expire 1 hour after being issued\n    {\n      role: current_user.role,\n      user_id: current_user.id.to_s,\n      exp: (Time.now + TOKEN_TTL).to_i\n    }\n  end\nend  \n     Token Time to Live \n     \n    In the code above we leverage the HTTP time based cache headers to expire the\n    endpoint cache at the same time as the token. In this example we have a token\n    that will be refresh one hour after its issuing time.\n    That's why both are based on the  TOKEN_TTL  constant.\n       We also need to create a route in the  config/routes.rb  file:  resource :api_token, only: [:show]  Now, any authenticated user in our rails application can request an api_token making a GET\nrequest to  /api_token . This endpoint will return a json object with one property\nwhose value is the token the API requests should use.", 
            "title": "Generating a JWT"
        }, 
        {
            "location": "/examples/external_auth/#orders-endpoint", 
            "text": "Here is how to create a view to generate an endpoint  /orders  filtered by\nthe logged in user:  ALTER DATABASE mydb SET postgrest.claims.user_id TO '';\n\nCREATE OR REPLACE FUNCTION current_user_id()\nRETURNS integer\nSTABLE\nLANGUAGE SQL\nAS $$\n    SELECT nullif(current_setting('postgrest.claims.user_id'), '')::integer;\n$$;\n\nCREATE SCHEMA private;\n\nCREATE TABLE private.orders (\n    id serial primary key,\n    user_id int references users,\n    created_at timestamp not null default current_timestamp,\n    updated_at timestamp not null default current_timestamp\n);\n\nCREATE VIEW orders AS\nSELECT\n    id, user_id, created_at, updated_at\nFROM\n    private.orders o\nWHERE\n    current_user = 'admin' OR o.user_id = current_user_id();  \n     DRY priviledge checking conditions \n     \n    You can encapsulate conditions that will be commonly used to check for privileges while reading a database row.\n    We used a function  current_user_id()  but we could add more conditions to functions\n    as the system becomes more complex. \n    Remeber to mark your functions as  STABLE  so that PostgreSQL can inline then while planning the query.", 
            "title": "Orders Endpoint"
        }, 
        {
            "location": "/examples/external_auth/#using-the-jwt", 
            "text": "Now whenever you are authenticated in your Rails application you can use some Javascript\n code to get the token and use it:  $.getJSON('/api_token').done(function(data){\n    $.ajax('/orders', {'Authorization': 'Bearer ' + data.token}).done(function(data){\n        console.log('Visible Orders: ', data);\n    })\n}).fail(function(){\n    console.log('Error fetching API token');\n})  We could also store the token to avoid having to fetch it again in the same page.", 
            "title": "Using the JWT"
        }, 
        {
            "location": "/examples/external_auth/#conclusion", 
            "text": "This section explained the implementation details for building an\nexternal authentication system working with PostgREST.\nWith the previous  User Management  example this should give a clearer\nidea of how to set up authentication for your API.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/examples/python-requests-jwt/", 
            "text": "Python Client for PostgREST API\n\n\nSetup PostgreSQL\n\n\nThis code relies on setting up the PostgreSQL auth functions and grants correctly first. Follow \nthese instructions\n.\n\n\nAfter completing the PostgreSQL configuration, be sure to create a user with email, password, role, and verified flag. We'll use that user to login in the code below.\n\n\nSetup PostgREST\n\n\nNext, setup PostgREST according to the documentation \nhttp://postgrest.com/install/server/\n.\n\n\nSetup Python Client\n\n\nFinally, we'll install and configure the python client. Follow the instructions in the \nREADME\n. Be sure to set the \ncredentials\n and \nurls\n in config.py.\n\n\nPython Client Functions\n\n\nThere are four primary functions to the python client:\n\n\n\n\nlogin\n\n\nconstruct_jwt_auth\n\n\nget_result_size\n\n\nget_range\n\n\n\n\nThe \nlogin\n and \nconstruct_jwt_auth\n functions will be required for any REST client using a PostgREST server, since a JWT auth instance is presumed.\n\n\nThe \nget_result_size\n and \nget_range\n functions are designed specifically for result sets where pagination is required. You can certainly use them for a single page result set that does not require pagination, but that may be overkill.\n\n\nLogin\n\n\nThe \nlogin function\n takes email and password strings (credentials.email and credentials.password, respectively from the config.py) and return the response.\n\n\nConstruct JWT Auth\n\n\nThe \nconstruct_jwt_auth\n function takes the auth response returned by the login function, retrieves the token in the response, and returns a JWT auth instance to the caller. The JWT auth instance can then be used for successive calls to the same PostgREST service.\n\n\nGet Result Size\n\n\nThe \nget_result_size\n function takes a JWT auth instance calls the URL at urls.data, extracts the size of the result set from the response object and returns the size.\n\n\nGet Range\n\n\nThe \nget_range\n function takes a beginning range, ending range, page size, and JWT auth instance, gets only that range of the available result set and returns JSON for that result set.", 
            "title": "Python Client"
        }, 
        {
            "location": "/examples/python-requests-jwt/#python-client-for-postgrest-api", 
            "text": "", 
            "title": "Python Client for PostgREST API"
        }, 
        {
            "location": "/examples/python-requests-jwt/#setup-postgresql", 
            "text": "This code relies on setting up the PostgreSQL auth functions and grants correctly first. Follow  these instructions .  After completing the PostgreSQL configuration, be sure to create a user with email, password, role, and verified flag. We'll use that user to login in the code below.", 
            "title": "Setup PostgreSQL"
        }, 
        {
            "location": "/examples/python-requests-jwt/#setup-postgrest", 
            "text": "Next, setup PostgREST according to the documentation  http://postgrest.com/install/server/ .", 
            "title": "Setup PostgREST"
        }, 
        {
            "location": "/examples/python-requests-jwt/#setup-python-client", 
            "text": "Finally, we'll install and configure the python client. Follow the instructions in the  README . Be sure to set the  credentials  and  urls  in config.py.", 
            "title": "Setup Python Client"
        }, 
        {
            "location": "/examples/python-requests-jwt/#python-client-functions", 
            "text": "There are four primary functions to the python client:   login  construct_jwt_auth  get_result_size  get_range   The  login  and  construct_jwt_auth  functions will be required for any REST client using a PostgREST server, since a JWT auth instance is presumed.  The  get_result_size  and  get_range  functions are designed specifically for result sets where pagination is required. You can certainly use them for a single page result set that does not require pagination, but that may be overkill.", 
            "title": "Python Client Functions"
        }, 
        {
            "location": "/examples/python-requests-jwt/#login", 
            "text": "The  login function  takes email and password strings (credentials.email and credentials.password, respectively from the config.py) and return the response.", 
            "title": "Login"
        }, 
        {
            "location": "/examples/python-requests-jwt/#construct-jwt-auth", 
            "text": "The  construct_jwt_auth  function takes the auth response returned by the login function, retrieves the token in the response, and returns a JWT auth instance to the caller. The JWT auth instance can then be used for successive calls to the same PostgREST service.", 
            "title": "Construct JWT Auth"
        }, 
        {
            "location": "/examples/python-requests-jwt/#get-result-size", 
            "text": "The  get_result_size  function takes a JWT auth instance calls the URL at urls.data, extracts the size of the result set from the response object and returns the size.", 
            "title": "Get Result Size"
        }, 
        {
            "location": "/examples/python-requests-jwt/#get-range", 
            "text": "The  get_range  function takes a beginning range, ending range, page size, and JWT auth instance, gets only that range of the available result set and returns JSON for that result set.", 
            "title": "Get Range"
        }
    ]
}